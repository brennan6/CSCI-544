{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5cb6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import warnings\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4924631",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data:\n",
    "tr_headers = [\"index\", \"word\", \"ner_tag\"]\n",
    "train_df = pd.read_csv(\"./data/train\", sep=' ', header=None, quoting=3)\n",
    "train_df.columns = tr_headers\n",
    "\n",
    "dev_df = pd.read_csv(\"./data/dev\", sep=' ', header=None, quoting=3)\n",
    "dev_df.columns = tr_headers\n",
    "\n",
    "test_headers = [\"index\", \"word\"]\n",
    "test_df = pd.read_csv(\"./data/test\", sep=' ', header=None, engine='python', error_bad_lines=False, quoting=3)\n",
    "test_df.columns = test_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03298400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>28</td>\n",
       "      <td>advice</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>29</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>30</td>\n",
       "      <td>clearer</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>31</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>We</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word ner_tag\n",
       "70     28   advice       O\n",
       "71     29      was       O\n",
       "72     30  clearer       O\n",
       "73     31        .       O\n",
       "74      1        \"       O\n",
       "75      2       We       O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[70:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d22a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>boycott</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>British</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>lamb</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Blackburn</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>BRUSSELS</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1996-08-22</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>European</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index        word ner_tag\n",
       "0       1          EU   B-ORG\n",
       "1       2     rejects       O\n",
       "2       3      German  B-MISC\n",
       "3       4        call       O\n",
       "4       5          to       O\n",
       "5       6     boycott       O\n",
       "6       7     British  B-MISC\n",
       "7       8        lamb       O\n",
       "8       9           .       O\n",
       "9       1       Peter   B-PER\n",
       "10      2   Blackburn   I-PER\n",
       "11      1    BRUSSELS   B-LOC\n",
       "12      2  1996-08-22       O\n",
       "13      1         The       O\n",
       "14      2    European   B-ORG"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99000c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight cleaning on num:\n",
    "train_df[\"word\"] = train_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "dev_df[\"word\"] = dev_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "test_df[\"word\"] = test_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5157aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of each word:\n",
    "#word-type = word\n",
    "cnt_d = {}\n",
    "for row in train_df.iterrows():\n",
    "    if row[1][\"word\"] in cnt_d:\n",
    "        cnt_d[row[1][\"word\"]] += 1\n",
    "    else:\n",
    "        cnt_d[row[1][\"word\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f0a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2 #No threshold = 1\n",
    "#unknown_cnt = 0\n",
    "unknown_word_lst = []   #We want to keep track of unknown words but group together\n",
    "for k, v in cnt_d.items():\n",
    "    if v < threshold:\n",
    "        #unknown_cnt += v\n",
    "        unknown_word_lst.append(k)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a660cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_train(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5340f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_dev(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    elif word not in train_words:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d8ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Train:\n",
    "train_df[\"word\"] = train_df[\"word\"].apply(replace_unk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b287a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word\"] = train_df[\"word\"].astype(str)\n",
    "train_vocab_size = len(np.unique(train_df[\"word\"]))\n",
    "train_words = np.unique(train_df[\"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68dde807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Dev:\n",
    "dev_df[\"word\"] = dev_df[\"word\"].apply(replace_unk_dev)\n",
    "dev_df[\"word\"] = dev_df[\"word\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816b2996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CRICKET</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TAKE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>OVER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51573</th>\n",
       "      <td>1</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51574</th>\n",
       "      <td>2</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51575</th>\n",
       "      <td>3</td>\n",
       "      <td>Newsroom</td>\n",
       "      <td>I-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51576</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;num&gt;-2&lt;num&gt;</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51577</th>\n",
       "      <td>1</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51578 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index          word ner_tag\n",
       "0          1       CRICKET       O\n",
       "1          2             -       O\n",
       "2          3         <unk>   B-ORG\n",
       "3          4          TAKE       O\n",
       "4          5          OVER       O\n",
       "...      ...           ...     ...\n",
       "51573      1            --       O\n",
       "51574      2         Dhaka   B-ORG\n",
       "51575      3      Newsroom   I-ORG\n",
       "51576      4  <num>-2<num>       O\n",
       "51577      1    -DOCSTART-       O\n",
       "\n",
       "[51578 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f83a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TRAIN:\n",
    "def format_data(df):\n",
    "    train_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word\"]]\n",
    "    sentence_y = [first_word[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        #print(row)\n",
    "        if row[1][\"index\"] == 1:\n",
    "            #print(row[1][\"word\"])\n",
    "            train_formatted.append([sentence_x, sentence_y])\n",
    "\n",
    "            sentence_x, sentence_y = [], []\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3409f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TEST:\n",
    "def format_data_test(df):\n",
    "    test_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(sentence_x)\n",
    "\n",
    "            sentence_x = []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d04a956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted = format_data(train_df)\n",
    "dev_formatted = format_data(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57f8914c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " 'Presidential': 1,\n",
       " 'Cuba': 2,\n",
       " 'Harrison': 3,\n",
       " 'Petah': 4,\n",
       " 'ineligible': 5,\n",
       " 'Ginebra': 6,\n",
       " 'Winner': 7,\n",
       " 'Kafelnikov': 8,\n",
       " 'Maynard': 9,\n",
       " 'Munton': 10,\n",
       " 'Leander': 11,\n",
       " 'offering': 12,\n",
       " 'nationalists': 13,\n",
       " 'Molenbeek': 14,\n",
       " 'Hollywood': 15,\n",
       " 'overcome': 16,\n",
       " 'Note': 17,\n",
       " 'devoted': 18,\n",
       " 'blow': 19,\n",
       " 'Perhaps': 20,\n",
       " 'Polish': 21,\n",
       " 'extortion': 22,\n",
       " 'Gopal': 23,\n",
       " 'ON': 24,\n",
       " 'background': 25,\n",
       " 'liberal': 26,\n",
       " 'northern': 27,\n",
       " 'Das': 28,\n",
       " 'Bystrica': 29,\n",
       " 'furlongs': 30,\n",
       " '.': 31,\n",
       " 'billions': 32,\n",
       " 'say': 33,\n",
       " 'contenders': 34,\n",
       " 'Hinckley': 35,\n",
       " 'continue': 36,\n",
       " 'buys': 37,\n",
       " 'two-goal': 38,\n",
       " 'depressed': 39,\n",
       " 'TWA': 40,\n",
       " 'combatants': 41,\n",
       " 'DIV': 42,\n",
       " 'carrying': 43,\n",
       " 'protects': 44,\n",
       " 'storming': 45,\n",
       " 'slammed': 46,\n",
       " 'some': 47,\n",
       " 'Mullally': 48,\n",
       " 'Israel': 49,\n",
       " 'resumed': 50,\n",
       " 'ANKARA': 51,\n",
       " 'Soyoil': 52,\n",
       " 'bushland': 53,\n",
       " 'claimed': 54,\n",
       " 'mouth': 55,\n",
       " 'focusing': 56,\n",
       " 'secretary-general': 57,\n",
       " 'run-scoring': 58,\n",
       " 'Elect': 59,\n",
       " 'Adam': 60,\n",
       " 'weathered': 61,\n",
       " 'quell': 62,\n",
       " 'Eldred': 63,\n",
       " 'Georgia': 64,\n",
       " 'Natasha': 65,\n",
       " 'marathon': 66,\n",
       " 'fill': 67,\n",
       " 'criminal': 68,\n",
       " 'violations': 69,\n",
       " 'Unknown': 70,\n",
       " 'two-week': 71,\n",
       " 'employer': 72,\n",
       " 'bat': 73,\n",
       " 'exchanges': 74,\n",
       " 'SHANGHAI': 75,\n",
       " 'taught': 76,\n",
       " 'Morinville': 77,\n",
       " 'excellent': 78,\n",
       " 'shattered': 79,\n",
       " 'fail': 80,\n",
       " 'Barzani': 81,\n",
       " 'Boskalis': 82,\n",
       " 'promises': 83,\n",
       " 'Masood': 84,\n",
       " 'gloom': 85,\n",
       " 'breakdown': 86,\n",
       " 'Tonga': 87,\n",
       " 'sides': 88,\n",
       " 'Rally': 89,\n",
       " 'Captain': 90,\n",
       " 'mill': 91,\n",
       " 'Coughlan': 92,\n",
       " 'hired': 93,\n",
       " 'hardliners': 94,\n",
       " 'Such': 95,\n",
       " 'Wisconsin': 96,\n",
       " 'affects': 97,\n",
       " 'passing': 98,\n",
       " 'landmark': 99,\n",
       " 'Highs': 100,\n",
       " 'payable': 101,\n",
       " 'Tutsis': 102,\n",
       " 'deserves': 103,\n",
       " 'Venezuelan': 104,\n",
       " 'mile': 105,\n",
       " 'Whittier': 106,\n",
       " 'Some': 107,\n",
       " 'aluminium': 108,\n",
       " 'Taleban': 109,\n",
       " 'cement': 110,\n",
       " 'southeastern': 111,\n",
       " 'MON': 112,\n",
       " 'Ken': 113,\n",
       " 'washed': 114,\n",
       " 'allied': 115,\n",
       " 'apologised': 116,\n",
       " 'Straits': 117,\n",
       " 'Lopez': 118,\n",
       " 'pen': 119,\n",
       " 'Montolio': 120,\n",
       " 'unknown': 121,\n",
       " '<num>-seat': 122,\n",
       " 'anxieties': 123,\n",
       " 'They': 124,\n",
       " 'Phelan': 125,\n",
       " 'Swiss': 126,\n",
       " 'offenders': 127,\n",
       " 'priority': 128,\n",
       " 'every': 129,\n",
       " 'action': 130,\n",
       " 'overs-a-side': 131,\n",
       " 'closures': 132,\n",
       " 'Gama': 133,\n",
       " 'Conchita': 134,\n",
       " 'workers': 135,\n",
       " 'den': 136,\n",
       " 'ISO': 137,\n",
       " 'commissioner': 138,\n",
       " 'military': 139,\n",
       " 'line': 140,\n",
       " 'decisive': 141,\n",
       " 'Heinrich': 142,\n",
       " 'caps': 143,\n",
       " 'Khalq': 144,\n",
       " 'rubber': 145,\n",
       " 'decibels': 146,\n",
       " 'Mechelen': 147,\n",
       " 'organised': 148,\n",
       " 'Everyone': 149,\n",
       " 'cancelled': 150,\n",
       " 'Fernandez': 151,\n",
       " 'Berkshire': 152,\n",
       " 'united': 153,\n",
       " 'clear': 154,\n",
       " 'base': 155,\n",
       " 'Formula': 156,\n",
       " 'electoral': 157,\n",
       " 'recover': 158,\n",
       " 'reward': 159,\n",
       " 'elephant': 160,\n",
       " 'electronics': 161,\n",
       " 'intense': 162,\n",
       " 'models': 163,\n",
       " 'Black': 164,\n",
       " 'emphasis': 165,\n",
       " 'J': 166,\n",
       " 'two': 167,\n",
       " 'OUT': 168,\n",
       " 'Veracruz': 169,\n",
       " 'RMT': 170,\n",
       " 'financial': 171,\n",
       " 'Isaac': 172,\n",
       " 'venue': 173,\n",
       " 'CUP': 174,\n",
       " 'PREMIUM': 175,\n",
       " 'probing': 176,\n",
       " 'dealt': 177,\n",
       " 'federal': 178,\n",
       " 'estimates': 179,\n",
       " 'taking': 180,\n",
       " 'likened': 181,\n",
       " 'overwhelmingly': 182,\n",
       " 'mainly': 183,\n",
       " 'shooting': 184,\n",
       " 'Moura': 185,\n",
       " 'brilliant': 186,\n",
       " 'Shi-Ting': 187,\n",
       " 'detective': 188,\n",
       " 'shotgun': 189,\n",
       " 'aggregate': 190,\n",
       " 'mine': 191,\n",
       " 'guerrillas': 192,\n",
       " 'estate': 193,\n",
       " 'wrote': 194,\n",
       " 'pure': 195,\n",
       " 'Muratovic': 196,\n",
       " 'Farm': 197,\n",
       " 'Philip': 198,\n",
       " 'BUSINESS': 199,\n",
       " 'follow-through': 200,\n",
       " 'oblige': 201,\n",
       " 'Horvath': 202,\n",
       " 'factors': 203,\n",
       " 'Casey': 204,\n",
       " 'Baird': 205,\n",
       " 'Pires': 206,\n",
       " 'quitting': 207,\n",
       " 'Gupta': 208,\n",
       " 'Ismail': 209,\n",
       " 'Yedioth': 210,\n",
       " 'retail': 211,\n",
       " 'area': 212,\n",
       " 'include': 213,\n",
       " 'pilot': 214,\n",
       " 'times': 215,\n",
       " 'Smicer': 216,\n",
       " 'Jacob': 217,\n",
       " 'patrol': 218,\n",
       " '$': 219,\n",
       " 'tbf': 220,\n",
       " 'Jamaica': 221,\n",
       " 'Old': 222,\n",
       " 'training': 223,\n",
       " 'raw': 224,\n",
       " 'Misael': 225,\n",
       " 'formal': 226,\n",
       " 'preparation': 227,\n",
       " 'moral': 228,\n",
       " 'candidate': 229,\n",
       " 'Tanui': 230,\n",
       " 'Francesco': 231,\n",
       " 'Germain': 232,\n",
       " 'new': 233,\n",
       " 'part': 234,\n",
       " 'identified': 235,\n",
       " 'AEX': 236,\n",
       " 'Maruyama': 237,\n",
       " 'OCEI': 238,\n",
       " 'findings': 239,\n",
       " 'million': 240,\n",
       " 'deny': 241,\n",
       " 'Timisoara': 242,\n",
       " 'advance': 243,\n",
       " 'five-run': 244,\n",
       " 'Santos': 245,\n",
       " 'patient': 246,\n",
       " 'partition': 247,\n",
       " 'nations': 248,\n",
       " 'socialist': 249,\n",
       " 'silence': 250,\n",
       " 'causing': 251,\n",
       " 'Arsenal': 252,\n",
       " 'comfortable': 253,\n",
       " 'jumps': 254,\n",
       " 'gone': 255,\n",
       " 'boards': 256,\n",
       " 'register': 257,\n",
       " 'bounce': 258,\n",
       " 'Bogota': 259,\n",
       " 'clocked': 260,\n",
       " '<num>-day': 261,\n",
       " 'Arthur': 262,\n",
       " 'Buzuku': 263,\n",
       " 'auctions': 264,\n",
       " 'maker': 265,\n",
       " 'everything': 266,\n",
       " 'lineup': 267,\n",
       " '<num>,600-12<num>': 268,\n",
       " 'Others': 269,\n",
       " 'Sanath': 270,\n",
       " 'ferrying': 271,\n",
       " 'shelled': 272,\n",
       " 'Boland': 273,\n",
       " 'immune': 274,\n",
       " 'quoting': 275,\n",
       " 'Amrollah': 276,\n",
       " 'Groningen': 277,\n",
       " 'retained': 278,\n",
       " 'fixture': 279,\n",
       " 'Wagner': 280,\n",
       " '<num>:38<num>': 281,\n",
       " 'script': 282,\n",
       " 'reduce': 283,\n",
       " 'disputed': 284,\n",
       " 'rise': 285,\n",
       " 'Gilbert': 286,\n",
       " 'coach': 287,\n",
       " 'Brunswijk': 288,\n",
       " 'In': 289,\n",
       " 'feel': 290,\n",
       " 'Otelul': 291,\n",
       " 'surgical': 292,\n",
       " 'square': 293,\n",
       " 'Dick': 294,\n",
       " 'outbreak': 295,\n",
       " 'late': 296,\n",
       " 'Rostock': 297,\n",
       " 'Srinath': 298,\n",
       " 'Stafford': 299,\n",
       " 'Shabana': 300,\n",
       " 'Arequipa': 301,\n",
       " 'Antioquia': 302,\n",
       " 'Wigan': 303,\n",
       " 'tense': 304,\n",
       " 'Vialle': 305,\n",
       " 'plainclothes': 306,\n",
       " 'striking': 307,\n",
       " 'inches': 308,\n",
       " 'Colin': 309,\n",
       " 'Maarten': 310,\n",
       " 'Nations': 311,\n",
       " \"'ll\": 312,\n",
       " 'theft': 313,\n",
       " 'doors': 314,\n",
       " 'strikers': 315,\n",
       " 'submit': 316,\n",
       " 'INDIA': 317,\n",
       " 'Vyacheslav': 318,\n",
       " 'massacre': 319,\n",
       " 'bed': 320,\n",
       " 'disease': 321,\n",
       " 'TYPE': 322,\n",
       " 'Yayuk': 323,\n",
       " 'Grande': 324,\n",
       " 'FORCE': 325,\n",
       " 'ahead': 326,\n",
       " 'Testud': 327,\n",
       " 'Croft': 328,\n",
       " 'Lanka': 329,\n",
       " 'nearly': 330,\n",
       " 'questions': 331,\n",
       " '<num>:29<num>': 332,\n",
       " 'Kerkrade': 333,\n",
       " 'Stomil': 334,\n",
       " 'ISLAMABAD': 335,\n",
       " 'boundary': 336,\n",
       " 'Jadeja': 337,\n",
       " 'extradition': 338,\n",
       " 'Karem': 339,\n",
       " 'through': 340,\n",
       " 'FC': 341,\n",
       " 'Church': 342,\n",
       " 'undermine': 343,\n",
       " 'NASDAQ': 344,\n",
       " 'dates': 345,\n",
       " 'gave': 346,\n",
       " 'Cuban': 347,\n",
       " 'DDF': 348,\n",
       " 'STOCKS': 349,\n",
       " 'Chechens': 350,\n",
       " 'cruised': 351,\n",
       " 'Awami': 352,\n",
       " 'Kamaz': 353,\n",
       " 'cup': 354,\n",
       " 'Numerous': 355,\n",
       " 'Rubin': 356,\n",
       " 'Domingo': 357,\n",
       " 'Liechenstein': 358,\n",
       " 'ITALIAN': 359,\n",
       " 'air': 360,\n",
       " 'apply': 361,\n",
       " 'Passengers': 362,\n",
       " 'belongs': 363,\n",
       " 'smashed': 364,\n",
       " 'Nicky': 365,\n",
       " 're-examine': 366,\n",
       " 'HOCKEY': 367,\n",
       " 'payroll': 368,\n",
       " 'numbers': 369,\n",
       " 'Palace': 370,\n",
       " 'agree': 371,\n",
       " 'Its': 372,\n",
       " 'parted': 373,\n",
       " 'Mushtaq': 374,\n",
       " 'Vladikavkaz': 375,\n",
       " 'problem': 376,\n",
       " 'honours': 377,\n",
       " 'hardly': 378,\n",
       " 'exports': 379,\n",
       " 'successful': 380,\n",
       " 'bowed': 381,\n",
       " 'Nunthorpe': 382,\n",
       " 'Dominic': 383,\n",
       " 'nabs': 384,\n",
       " 'Stich': 385,\n",
       " 'Up': 386,\n",
       " 'field': 387,\n",
       " 'Joel': 388,\n",
       " 'Cement': 389,\n",
       " 'Benin': 390,\n",
       " 'complex': 391,\n",
       " 'starting': 392,\n",
       " 'four-year': 393,\n",
       " 'Pascal': 394,\n",
       " 'visited': 395,\n",
       " 'Marsh': 396,\n",
       " 'PMC-Sierra': 397,\n",
       " 'Zicot': 398,\n",
       " 'hurricane': 399,\n",
       " 'blaze': 400,\n",
       " 'Berlin': 401,\n",
       " 'focus': 402,\n",
       " 'lawyer': 403,\n",
       " 'Department': 404,\n",
       " 'quarter-finals': 405,\n",
       " 'rarely': 406,\n",
       " 'tender': 407,\n",
       " 'Lecce': 408,\n",
       " 'stabilise': 409,\n",
       " 'aid': 410,\n",
       " 'Novorossiisk': 411,\n",
       " 'examination': 412,\n",
       " 'computer': 413,\n",
       " 'slipping': 414,\n",
       " 'DE': 415,\n",
       " 'dismissed': 416,\n",
       " 'suspended': 417,\n",
       " 'Tour': 418,\n",
       " 'grew': 419,\n",
       " 'managed': 420,\n",
       " 'Simpson': 421,\n",
       " 'Maureen': 422,\n",
       " 'lose': 423,\n",
       " 'tents': 424,\n",
       " 'Lejeune': 425,\n",
       " 'anger': 426,\n",
       " 'MT': 427,\n",
       " 'confiscated': 428,\n",
       " 'CHAMPIONS': 429,\n",
       " 'bright': 430,\n",
       " 'Frankie': 431,\n",
       " 'manage': 432,\n",
       " 'surrender': 433,\n",
       " 'Federation': 434,\n",
       " 'Neither': 435,\n",
       " 'PRICE': 436,\n",
       " 'Wilander': 437,\n",
       " 'Chandra': 438,\n",
       " 'Leaders': 439,\n",
       " 'Takuma': 440,\n",
       " 'collateral': 441,\n",
       " 'Amoco': 442,\n",
       " 'often': 443,\n",
       " 'karbovanets': 444,\n",
       " 'Penh': 445,\n",
       " 'Kikinda': 446,\n",
       " 'aboard': 447,\n",
       " 'sparked': 448,\n",
       " 'Morris': 449,\n",
       " 'Madrid': 450,\n",
       " 'Hedblom': 451,\n",
       " 'Jaha': 452,\n",
       " 'UNION': 453,\n",
       " 'trend': 454,\n",
       " 'Bukovec': 455,\n",
       " 'Nacar': 456,\n",
       " 'rewarded': 457,\n",
       " 'absolute': 458,\n",
       " 'Carnival': 459,\n",
       " 'BALTIMORE': 460,\n",
       " 'stolen': 461,\n",
       " 'links': 462,\n",
       " 'ninth': 463,\n",
       " 'teen': 464,\n",
       " 'Lebouc': 465,\n",
       " 'DELHI': 466,\n",
       " 'Dhahran': 467,\n",
       " 'medium': 468,\n",
       " 'forms': 469,\n",
       " 'interval': 470,\n",
       " 'units': 471,\n",
       " 'Florencia': 472,\n",
       " 'Alaska': 473,\n",
       " 'read': 474,\n",
       " 'visit': 475,\n",
       " 'no-confidence': 476,\n",
       " \"O'Leary\": 477,\n",
       " 'Chinese': 478,\n",
       " 'Jerome': 479,\n",
       " 'Labat': 480,\n",
       " 'Geneva': 481,\n",
       " 'Brighton': 482,\n",
       " 'Sarah': 483,\n",
       " 'really': 484,\n",
       " 'Alfred': 485,\n",
       " '<num>/8<num>': 486,\n",
       " 'ENDS': 487,\n",
       " '<num>,583<num>': 488,\n",
       " 'Andre': 489,\n",
       " 'Delhi': 490,\n",
       " 'ankle': 491,\n",
       " 'Tauziat': 492,\n",
       " 'imports': 493,\n",
       " 'Park': 494,\n",
       " 'guitar': 495,\n",
       " 'rings': 496,\n",
       " 'Average': 497,\n",
       " 'Durham': 498,\n",
       " 'strategist': 499,\n",
       " 'lists': 500,\n",
       " 'Crawley': 501,\n",
       " 'Syrian': 502,\n",
       " 'Liberation': 503,\n",
       " 'T-bill': 504,\n",
       " 'staged': 505,\n",
       " 'knows': 506,\n",
       " 'Aynaoui': 507,\n",
       " 'Including': 508,\n",
       " 'Gibbs': 509,\n",
       " 'KV': 510,\n",
       " 'Diniz': 511,\n",
       " 'Athletics': 512,\n",
       " 'province': 513,\n",
       " 'rolls': 514,\n",
       " '<num>-mile': 515,\n",
       " 'checked': 516,\n",
       " 'Hick': 517,\n",
       " 'stabbed': 518,\n",
       " 'occupy': 519,\n",
       " 'Jason': 520,\n",
       " 'Between': 521,\n",
       " 'confusing': 522,\n",
       " 'overall': 523,\n",
       " 'fell': 524,\n",
       " 'MIAMI': 525,\n",
       " 'broker': 526,\n",
       " 'Saskatchewan': 527,\n",
       " 'priest': 528,\n",
       " '<num>:06<num>': 529,\n",
       " 'Fleming': 530,\n",
       " 'COMEX': 531,\n",
       " 'From': 532,\n",
       " 'sued': 533,\n",
       " 'over': 534,\n",
       " 'likely': 535,\n",
       " 'Husqvarna': 536,\n",
       " 'old': 537,\n",
       " 'vowed': 538,\n",
       " 'videos': 539,\n",
       " 'Taiwanese': 540,\n",
       " 'riyal': 541,\n",
       " 'simply': 542,\n",
       " 'NSDAP-AO': 543,\n",
       " 'goalkeeper': 544,\n",
       " 'storage': 545,\n",
       " 'changed': 546,\n",
       " 'defenders': 547,\n",
       " 'Fremantle': 548,\n",
       " 'finding': 549,\n",
       " 'remaining': 550,\n",
       " 'NOTES': 551,\n",
       " 'Chicago': 552,\n",
       " 'rolled': 553,\n",
       " 'motor': 554,\n",
       " 'COUNTY': 555,\n",
       " 'blind': 556,\n",
       " 'Portugal': 557,\n",
       " 'Suleyman': 558,\n",
       " 'arrests': 559,\n",
       " 'pre-abnormals': 560,\n",
       " 'Ben': 561,\n",
       " 'identities': 562,\n",
       " 'Mamiit': 563,\n",
       " 'Fischler': 564,\n",
       " 'livestock': 565,\n",
       " 'Prinosil': 566,\n",
       " 'press': 567,\n",
       " 'transport': 568,\n",
       " 'EU': 569,\n",
       " 'superbike': 570,\n",
       " 'Collective': 571,\n",
       " 'shipments': 572,\n",
       " 'Gurusinha': 573,\n",
       " 'Damm': 574,\n",
       " 'Empoli': 575,\n",
       " 'entertaining': 576,\n",
       " 'undercover': 577,\n",
       " 'stadium': 578,\n",
       " \"O'Meara\": 579,\n",
       " 'Taiwan': 580,\n",
       " 'Market': 581,\n",
       " 'Vatican': 582,\n",
       " 'troop': 583,\n",
       " 'longitudinal': 584,\n",
       " 'Heinz-Harald': 585,\n",
       " '<num>:28<num>': 586,\n",
       " 'importance': 587,\n",
       " 'implemented': 588,\n",
       " 'Vivit': 589,\n",
       " 'tap': 590,\n",
       " 'River': 591,\n",
       " 'Pudvah': 592,\n",
       " 'AEI': 593,\n",
       " 'Right': 594,\n",
       " 'CBI': 595,\n",
       " 'Rishon': 596,\n",
       " 'snap': 597,\n",
       " 'tampering': 598,\n",
       " 'Bay': 599,\n",
       " 'Laboratories': 600,\n",
       " 'Postal': 601,\n",
       " 'Blijlevens': 602,\n",
       " 'local': 603,\n",
       " 'Sary': 604,\n",
       " 'Rui': 605,\n",
       " 'PSG': 606,\n",
       " 'exchange': 607,\n",
       " 'Masterkova': 608,\n",
       " 'islands': 609,\n",
       " 'jackpot': 610,\n",
       " 'nervousness': 611,\n",
       " 'Constructorul': 612,\n",
       " 'various': 613,\n",
       " 'Tanzania': 614,\n",
       " 'Doug': 615,\n",
       " 'schools': 616,\n",
       " 'rifles': 617,\n",
       " 'coached': 618,\n",
       " 'expires': 619,\n",
       " 'dead': 620,\n",
       " 'batted': 621,\n",
       " 'seek': 622,\n",
       " 'Costantino': 623,\n",
       " 'Mozambique': 624,\n",
       " 'debt': 625,\n",
       " 'welcomed': 626,\n",
       " 'arrive': 627,\n",
       " 'exclude': 628,\n",
       " 'OAKLAND': 629,\n",
       " 'Bjorkman': 630,\n",
       " 'atrocities': 631,\n",
       " 'BP': 632,\n",
       " 'taxes': 633,\n",
       " 'gloomy': 634,\n",
       " 'develop': 635,\n",
       " 'MPs': 636,\n",
       " 'backwardation': 637,\n",
       " 'Names': 638,\n",
       " 'displayed': 639,\n",
       " 'GENEVA': 640,\n",
       " 'rural': 641,\n",
       " 'Jiul': 642,\n",
       " 'Monetary': 643,\n",
       " 'indications': 644,\n",
       " 'Doboj': 645,\n",
       " 'Even': 646,\n",
       " 'servants': 647,\n",
       " 'Rajavi': 648,\n",
       " 'help': 649,\n",
       " 'secretary': 650,\n",
       " 'Flamengo': 651,\n",
       " 'damaged': 652,\n",
       " 'Aamir': 653,\n",
       " 'latter': 654,\n",
       " 'driving': 655,\n",
       " 'A$': 656,\n",
       " 'Ray': 657,\n",
       " 'congressmen': 658,\n",
       " 'politicians': 659,\n",
       " 'quite': 660,\n",
       " 'Maybe': 661,\n",
       " 'roads': 662,\n",
       " 'Kinross': 663,\n",
       " 'third-set': 664,\n",
       " 'blame': 665,\n",
       " 'Eddie': 666,\n",
       " 'extrusion': 667,\n",
       " 'offences': 668,\n",
       " 'MQM': 669,\n",
       " 'reflects': 670,\n",
       " 'join': 671,\n",
       " 'confirmation': 672,\n",
       " 'accordance': 673,\n",
       " 'highlight': 674,\n",
       " 'sustainable': 675,\n",
       " 'considered': 676,\n",
       " 'Nicaraguan': 677,\n",
       " 'playing': 678,\n",
       " 'plunged': 679,\n",
       " 'jailed': 680,\n",
       " 'brutal': 681,\n",
       " 'Taument': 682,\n",
       " 'Facilities': 683,\n",
       " 'Rock': 684,\n",
       " 'bankruptcy': 685,\n",
       " 'Alliance': 686,\n",
       " 'representing': 687,\n",
       " 'Davis': 688,\n",
       " 'Schett': 689,\n",
       " 'looks': 690,\n",
       " 'INDEX': 691,\n",
       " 'Georgiopoulos': 692,\n",
       " 'sixth-seeded': 693,\n",
       " 'variable': 694,\n",
       " 'HELENS': 695,\n",
       " 'Constanta': 696,\n",
       " 'Reds': 697,\n",
       " 'Freiburg': 698,\n",
       " 'Rosset': 699,\n",
       " 'NICOSIA': 700,\n",
       " 'removed': 701,\n",
       " 'stalled': 702,\n",
       " 'winner': 703,\n",
       " 'providing': 704,\n",
       " 'technical': 705,\n",
       " 'maybe': 706,\n",
       " 'involvement': 707,\n",
       " 'deposed': 708,\n",
       " 'forever': 709,\n",
       " 'Land': 710,\n",
       " 'convinced': 711,\n",
       " 'analysis': 712,\n",
       " 'volcano': 713,\n",
       " 'al-Kabariti': 714,\n",
       " 'walks': 715,\n",
       " 'failure': 716,\n",
       " 'Pool': 717,\n",
       " 'recovering': 718,\n",
       " 'Ijaz': 719,\n",
       " 'succeeded': 720,\n",
       " 'corporate': 721,\n",
       " 'Boca': 722,\n",
       " 'indifference': 723,\n",
       " 'Jeremy': 724,\n",
       " 'Kabul': 725,\n",
       " 'Rate': 726,\n",
       " 'conscious': 727,\n",
       " 'seeing': 728,\n",
       " 'Boutros-Ghali': 729,\n",
       " 'Pa': 730,\n",
       " 'Jail': 731,\n",
       " 'real': 732,\n",
       " 'variety': 733,\n",
       " ';': 734,\n",
       " 'Home': 735,\n",
       " 'Itamar': 736,\n",
       " 'long': 737,\n",
       " 'RECORD': 738,\n",
       " 'Energy': 739,\n",
       " 'sister': 740,\n",
       " 'smugglers': 741,\n",
       " 'momentum': 742,\n",
       " 'Anders': 743,\n",
       " 'Barcelona': 744,\n",
       " 'Delivery': 745,\n",
       " 'tells': 746,\n",
       " 'Reeve': 747,\n",
       " 'England': 748,\n",
       " 'Alicia': 749,\n",
       " 'continued': 750,\n",
       " 'giant': 751,\n",
       " 'FDA': 752,\n",
       " 'canal': 753,\n",
       " 'adequate': 754,\n",
       " 'Fulham': 755,\n",
       " 'Auckland': 756,\n",
       " 'Nicaragua': 757,\n",
       " 'resigned': 758,\n",
       " 'victims': 759,\n",
       " 'TENNIS': 760,\n",
       " 'whose': 761,\n",
       " 'Ronaldo': 762,\n",
       " 'voiced': 763,\n",
       " 'Mind': 764,\n",
       " 'Food': 765,\n",
       " 'Jones': 766,\n",
       " 'noble': 767,\n",
       " 'still': 768,\n",
       " 'trillion': 769,\n",
       " 'AUGUST': 770,\n",
       " 'Shipping': 771,\n",
       " 'strong': 772,\n",
       " 'played': 773,\n",
       " 'environment': 774,\n",
       " 'Lilly': 775,\n",
       " 'DIARIO': 776,\n",
       " 'only': 777,\n",
       " 'founder': 778,\n",
       " 'penultimate': 779,\n",
       " 'ancient': 780,\n",
       " 'LAW': 781,\n",
       " 'Hamburg': 782,\n",
       " 'cultures': 783,\n",
       " 'showers': 784,\n",
       " 'Avonex': 785,\n",
       " 'current': 786,\n",
       " 'Frederic': 787,\n",
       " 'Monterrey': 788,\n",
       " 'soldier': 789,\n",
       " 'FI': 790,\n",
       " 'note': 791,\n",
       " 'offset': 792,\n",
       " 'crazy': 793,\n",
       " 'Grant': 794,\n",
       " 'School': 795,\n",
       " 'dwt': 796,\n",
       " 'Sakigake': 797,\n",
       " 'merchants': 798,\n",
       " 'definitive': 799,\n",
       " 'procure': 800,\n",
       " 'riding': 801,\n",
       " 'League': 802,\n",
       " 'DENOMS': 803,\n",
       " 'preliminary': 804,\n",
       " 'S': 805,\n",
       " 'boatman': 806,\n",
       " 'Greens': 807,\n",
       " 'forecast': 808,\n",
       " 'MIDEAST': 809,\n",
       " 'protection': 810,\n",
       " 'Tommi': 811,\n",
       " 'searching': 812,\n",
       " 'Wijffels': 813,\n",
       " 'prominent': 814,\n",
       " 'Tim': 815,\n",
       " 'equal': 816,\n",
       " 'Eritrean': 817,\n",
       " 'semifinalist': 818,\n",
       " 'Mutola': 819,\n",
       " 'heavier': 820,\n",
       " 'Tami': 821,\n",
       " 'gets': 822,\n",
       " 'prove': 823,\n",
       " 'Bernard': 824,\n",
       " 'Humberto': 825,\n",
       " '<num>,901<num>': 826,\n",
       " 'Goulnara': 827,\n",
       " 'murders': 828,\n",
       " 'Post': 829,\n",
       " 'opportunities': 830,\n",
       " 'passage': 831,\n",
       " 'The': 832,\n",
       " 'liquids': 833,\n",
       " 'table': 834,\n",
       " 'Cubs': 835,\n",
       " 'Mrs.': 836,\n",
       " 'Standard': 837,\n",
       " 'Jack': 838,\n",
       " 'administrative': 839,\n",
       " 'benefit': 840,\n",
       " 'status': 841,\n",
       " 'Manchester': 842,\n",
       " 'outlook': 843,\n",
       " 'allege': 844,\n",
       " 'parent': 845,\n",
       " 'seeded': 846,\n",
       " 'Romano': 847,\n",
       " 'MONEY': 848,\n",
       " 'Affairs': 849,\n",
       " 'LUNCH': 850,\n",
       " 'Universitatea': 851,\n",
       " 'selected': 852,\n",
       " 'trail': 853,\n",
       " 'VanLandingham': 854,\n",
       " 'altered': 855,\n",
       " 'claims': 856,\n",
       " 'described': 857,\n",
       " 'agreed': 858,\n",
       " 'Schaller': 859,\n",
       " 'Chonan': 860,\n",
       " 'bases': 861,\n",
       " 'Ronald': 862,\n",
       " 'fake': 863,\n",
       " 'corner': 864,\n",
       " 'Start': 865,\n",
       " 'options': 866,\n",
       " 'Kandahar': 867,\n",
       " 'protested': 868,\n",
       " 'cushion': 869,\n",
       " 'smoothly': 870,\n",
       " 'Michael': 871,\n",
       " 'publicly': 872,\n",
       " 'destroyed': 873,\n",
       " 'unspecified': 874,\n",
       " 'gestures': 875,\n",
       " 'communist': 876,\n",
       " 'manager': 877,\n",
       " 'Jacek': 878,\n",
       " 'Agreement': 879,\n",
       " 'solvent': 880,\n",
       " 'ACC': 881,\n",
       " 'gluten': 882,\n",
       " 'stealing': 883,\n",
       " 'Corrigendum': 884,\n",
       " 'top': 885,\n",
       " 'vows': 886,\n",
       " 'MOSCOW': 887,\n",
       " 'rupiah': 888,\n",
       " 'Ellwood': 889,\n",
       " 'Toronto-based': 890,\n",
       " 'poor': 891,\n",
       " 'Sampras': 892,\n",
       " 'tighten': 893,\n",
       " 'Georgalis': 894,\n",
       " 'unable': 895,\n",
       " '<num>,700-12<num>': 896,\n",
       " 'strengthen': 897,\n",
       " 'Mariners': 898,\n",
       " 'Gross': 899,\n",
       " 'mountainous': 900,\n",
       " 'bacteria': 901,\n",
       " 'Refining': 902,\n",
       " 'lone': 903,\n",
       " 'Sosa': 904,\n",
       " 'suitable': 905,\n",
       " 'freestyle': 906,\n",
       " 'LG': 907,\n",
       " 'AFRICAN': 908,\n",
       " 'Joost': 909,\n",
       " 'expansion': 910,\n",
       " 'YEAR-AGO': 911,\n",
       " 'SQUAD': 912,\n",
       " 'one-time': 913,\n",
       " 'unlucky': 914,\n",
       " 'strikeouts': 915,\n",
       " 'Cardiff': 916,\n",
       " 'After': 917,\n",
       " 'editorial': 918,\n",
       " 'fiasco': 919,\n",
       " 'Partners': 920,\n",
       " 'pain': 921,\n",
       " 'Crescent': 922,\n",
       " 'report': 923,\n",
       " 'reports': 924,\n",
       " 'bondholders': 925,\n",
       " 'Quarterly': 926,\n",
       " 'Leone': 927,\n",
       " 'suffered': 928,\n",
       " 'Revenue': 929,\n",
       " 'Leon': 930,\n",
       " 'Muzaffarabad': 931,\n",
       " 'Ian': 932,\n",
       " 'Schumacher': 933,\n",
       " 'Zaman': 934,\n",
       " 'leaves': 935,\n",
       " 'Ek': 936,\n",
       " 'mutton': 937,\n",
       " 'ATAGI': 938,\n",
       " 'TM': 939,\n",
       " 'surgery': 940,\n",
       " 'slumped': 941,\n",
       " 'Irish': 942,\n",
       " 'Zizkov': 943,\n",
       " 'day': 944,\n",
       " 'Cherbourg': 945,\n",
       " 'Nasdaq': 946,\n",
       " 'Rica': 947,\n",
       " 'lunch': 948,\n",
       " 'midfielder': 949,\n",
       " 'Hoch': 950,\n",
       " 'semifinals': 951,\n",
       " 'Cash': 952,\n",
       " 'disposable': 953,\n",
       " 'post-war': 954,\n",
       " 'Pantic': 955,\n",
       " 'comprehensive': 956,\n",
       " 'Lancashire': 957,\n",
       " 'hanged': 958,\n",
       " 'faded': 959,\n",
       " 'FULL': 960,\n",
       " 'unhurt': 961,\n",
       " 'Kamoga': 962,\n",
       " 'surprised': 963,\n",
       " 'weakened': 964,\n",
       " 'promoted': 965,\n",
       " 'noon': 966,\n",
       " 'feature': 967,\n",
       " 'positive': 968,\n",
       " 'string': 969,\n",
       " \"'m\": 970,\n",
       " 'Klas': 971,\n",
       " 'fate': 972,\n",
       " 'Queensland': 973,\n",
       " 'scores': 974,\n",
       " 'ROME': 975,\n",
       " 'Gaudenzi': 976,\n",
       " 'boxes': 977,\n",
       " 'Action': 978,\n",
       " 'Mr': 979,\n",
       " 'Ruutel': 980,\n",
       " 'PDI': 981,\n",
       " 'blamed': 982,\n",
       " 'runs': 983,\n",
       " 'shaken': 984,\n",
       " 'possibility': 985,\n",
       " 'uphill': 986,\n",
       " 'impression': 987,\n",
       " 'refunds': 988,\n",
       " 'expulsion': 989,\n",
       " 'Authorities': 990,\n",
       " 'near': 991,\n",
       " 'tennis': 992,\n",
       " 'Boutros': 993,\n",
       " 'production': 994,\n",
       " 'Sterling': 995,\n",
       " 'replied': 996,\n",
       " 'Southland': 997,\n",
       " 'suffers': 998,\n",
       " 'Hock': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Word Map for vocab:\n",
    "word_map = {\"<pad>\":0}\n",
    "for i, word in enumerate(set(train_df[\"word\"])):\n",
    "    word_map[word] = i+1\n",
    "word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e418bae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': -1,\n",
       " 'B-PER': 0,\n",
       " 'I-ORG': 1,\n",
       " 'O': 2,\n",
       " 'I-MISC': 3,\n",
       " 'B-MISC': 4,\n",
       " 'I-PER': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-LOC': 7,\n",
       " 'B-ORG': 8}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Word Map for ner_tag:\n",
    "ner_map = {\"<pad>\":-1}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map[word] = i\n",
    "ner_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34291cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-PER': 0,\n",
       " 'I-ORG': 1,\n",
       " 'O': 2,\n",
       " 'I-MISC': 3,\n",
       " 'B-MISC': 4,\n",
       " 'I-PER': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-LOC': 7,\n",
       " 'B-ORG': 8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_map_without_pad = {}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map_without_pad[word] = i\n",
    "ner_map_without_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f91bab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_train_sent = 0\n",
    "for sentence in train_formatted:\n",
    "    sentence_len = len(sentence[0])\n",
    "    if sentence_len > longest_train_sent:\n",
    "        longest_train_sent = sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ff4c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13f5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_sentences(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    print(cnt)\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82078275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567\n",
      "51578\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sentences(train_formatted)\n",
    "dev_padded = pad_sentences(dev_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f25e80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_test_sentences(sentences_formatted):\n",
    "    test_padded = []\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712824b2",
   "metadata": {},
   "source": [
    "### Task 1: Simple Bidirectional LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac7d7ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3466"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f662e835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14987"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b472a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "340df0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.shape)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        linear_out_dropped = self.dropout(out)\n",
    "        #l2_out = self.linear2(linear_out_dropped)\n",
    "        elu_out = self.elu(linear_out_dropped)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04322e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.061716 \tTest Loss: 0.053083\n",
      "Epoch: 2 \tTraining Loss: 0.048122 \tTest Loss: 0.046673\n",
      "Epoch: 3 \tTraining Loss: 0.039629 \tTest Loss: 0.039134\n",
      "Epoch: 4 \tTraining Loss: 0.033555 \tTest Loss: 0.033525\n",
      "Epoch: 5 \tTraining Loss: 0.024118 \tTest Loss: 0.028855\n",
      "Epoch: 6 \tTraining Loss: 0.020382 \tTest Loss: 0.027672\n",
      "Epoch: 7 \tTraining Loss: 0.019041 \tTest Loss: 0.027528\n",
      "Epoch: 8 \tTraining Loss: 0.018215 \tTest Loss: 0.026919\n",
      "Epoch: 9 \tTraining Loss: 0.017680 \tTest Loss: 0.026281\n",
      "Epoch: 10 \tTraining Loss: 0.016589 \tTest Loss: 0.026506\n",
      "Epoch: 11 \tTraining Loss: 0.016186 \tTest Loss: 0.026321\n",
      "Epoch: 12 \tTraining Loss: 0.015937 \tTest Loss: 0.025970\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-3c0bb7b96399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mblstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Zero the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCAB_SIZE = train_vocab_size+1 #added <pad> word\n",
    "n_epochs = 20\n",
    "trainloader = torch.utils.data.DataLoader(train_padded, batch_size=12, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=12, num_workers=1)\n",
    "blstm = BLSTM(VOCAB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(blstm.parameters(), lr=0.25, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm.state_dict(), 'blstm1.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0947aa58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-82746b271fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_formatted\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_formatted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_formatted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_data_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_test_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_formatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3c295e24e5b3>\u001b[0m in \u001b[0;36mformat_data_test\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msentence_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_formatted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mtrain_formatted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msentence_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word_formatted\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_y' is not defined"
     ]
    }
   ],
   "source": [
    "#Format the test_data:\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(replace_unk_dev)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted = format_data_test(test_df)\n",
    "test_padded = pad_test_sentences(test_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5434da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm.load_state_dict(torch.load('blstm1.pt'))\n",
    "#testloader = torch.utils.data.DataLoader(test_padded[0:100], batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0bb49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict_test(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e16a8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c8507d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions_test(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        non_padded_pred = len(np.nonzero(sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6859b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        actual_sentence = sentence[0]\n",
    "        non_padded_pred = len(np.nonzero(actual_sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d40a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(pred):\n",
    "    overall_pred = []\n",
    "    for sentence in pred:\n",
    "        for idx in sentence:\n",
    "            overall_pred.append(list(ner_map_without_pad.keys())[idx])\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f3f09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    score = sum(y_true == y_pred)/len(y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3d96f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "a28ce5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-LOC',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-PER',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-LOC',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'I-MISC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'I-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-PER',\n",
       " 'B-MISC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'B-LOC',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'I-MISC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-PER',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'I-LOC',\n",
       " 'I-LOC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'O',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " 'B-LOC',\n",
       " 'B-MISC',\n",
       " 'O',\n",
       " ...]"
      ]
     },
     "execution_count": 669,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6b77641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.9178331846911474\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Best score: lr=.1, momentum=.9, epoch=30, batch_size-64 - 91.77% acc, 55.89 F1, ~2hrs\n",
    "\n",
    "#Most promising: lr=.25, momentum=.9, epoch=10, batch_size=212 - 91.78%, 55.07 F1 ~45min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "569c4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(name, y_true, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(row[1][\"word\"])\n",
    "            f.write(\" \")\n",
    "            f.write(y_true[row[0]])\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea7d71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev1.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "0fdd3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test:\n",
    "predictions = predict_test(blstm, testloader)\n",
    "predictions = unravel_predictions_test(test_padded[0:100], predictions)\n",
    "predictions = convert_predictions(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771ad68",
   "metadata": {},
   "source": [
    "### Task 2: Using GloVe Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24788cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./glove.6B.100d\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    word2vec={}\n",
    "    for word_embedding in f:\n",
    "        word_split = word_embedding.split()\n",
    "        word = word_split[0]\n",
    "        word2vec[word] = np.array(word_split[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c914b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCAB_SIZE = train_vocab_size+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c1d01e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in word_map.items():\n",
    "    if word in word2vec:\n",
    "        word_embedding = word2vec[word]\n",
    "        embedding_matrix[idx,:] = word_embedding\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "embedding_blstm2 = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedding_blstm2.load_state_dict({\"weight\": torch.tensor(embedding_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87a3fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.shape)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        linear_out_dropped = self.dropout(out)\n",
    "        #l2_out = self.linear2(linear_out_dropped)\n",
    "        elu_out = self.elu(linear_out_dropped)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ecc8c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.25\n",
      "Epoch: 1 \tTraining Loss: 0.045912 \tTest Loss: 0.037665\n",
      "Epoch-1 lr: 0.25\n",
      "Epoch: 2 \tTraining Loss: 0.036792 \tTest Loss: 0.036313\n",
      "Epoch-2 lr: 0.25\n",
      "Epoch: 3 \tTraining Loss: 0.028793 \tTest Loss: 0.029757\n",
      "Epoch-3 lr: 0.25\n",
      "Epoch: 4 \tTraining Loss: 0.021845 \tTest Loss: 0.025563\n",
      "Epoch-4 lr: 0.25\n",
      "Epoch: 5 \tTraining Loss: 0.019020 \tTest Loss: 0.024352\n",
      "Epoch-5 lr: 0.25\n",
      "Epoch: 6 \tTraining Loss: 0.016951 \tTest Loss: 0.023817\n",
      "Epoch-6 lr: 0.25\n",
      "Epoch: 7 \tTraining Loss: 0.016222 \tTest Loss: 0.023508\n",
      "Epoch-7 lr: 0.25\n",
      "Epoch: 8 \tTraining Loss: 0.015854 \tTest Loss: 0.023464\n",
      "Epoch-8 lr: 0.25\n",
      "Epoch: 9 \tTraining Loss: 0.015519 \tTest Loss: 0.023307\n",
      "Epoch-9 lr: 0.125\n",
      "Epoch: 10 \tTraining Loss: 0.014464 \tTest Loss: 0.024054\n",
      "Epoch-10 lr: 0.125\n",
      "Epoch: 11 \tTraining Loss: 0.014135 \tTest Loss: 0.023769\n",
      "Epoch-11 lr: 0.125\n",
      "Epoch: 12 \tTraining Loss: 0.014027 \tTest Loss: 0.023911\n",
      "Epoch-12 lr: 0.125\n",
      "Epoch: 13 \tTraining Loss: 0.013873 \tTest Loss: 0.023952\n",
      "Epoch-13 lr: 0.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-a496499ea9c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Perform backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Perform optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCAB_SIZE = train_vocab_size+1 #added <pad> word\n",
    "n_epochs = 20\n",
    "trainloader = torch.utils.data.DataLoader(train_padded, batch_size=12, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=12, num_workers=1)\n",
    "blstm2 = BLSTM_2(embedding_blstm2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(blstm2.parameters(), lr=0.25, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm2.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm2(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm2(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm2.state_dict(), 'blstm2.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cafe50b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the best model from the given run:\n",
    "blstm2.load_state_dict(torch.load('blstm2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "583046b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm2, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "57dbb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.925045562061344\n"
     ]
    }
   ],
   "source": [
    "#Dev accuracy:\n",
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d7e2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev2.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN Command line:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
