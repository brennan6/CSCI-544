{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6c5cb6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import warnings\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c4924631",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data:\n",
    "tr_headers = [\"index\", \"word\", \"ner_tag\"]\n",
    "train_df = pd.read_csv(\"./data/train\", sep=' ', header=None, quoting=3)\n",
    "train_df.columns = tr_headers\n",
    "\n",
    "dev_df = pd.read_csv(\"./data/dev\", sep=' ', header=None, quoting=3)\n",
    "dev_df.columns = tr_headers\n",
    "\n",
    "test_headers = [\"index\", \"word\"]\n",
    "test_df = pd.read_csv(\"./data/test\", sep=' ', header=None, engine='python', error_bad_lines=False, quoting=3)\n",
    "test_df.columns = test_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "03298400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>28</td>\n",
       "      <td>advice</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>29</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>30</td>\n",
       "      <td>clearer</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>31</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>We</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word ner_tag\n",
       "70     28   advice       O\n",
       "71     29      was       O\n",
       "72     30  clearer       O\n",
       "73     31        .       O\n",
       "74      1        \"       O\n",
       "75      2       We       O"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[70:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "10d22a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>boycott</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>British</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>lamb</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Blackburn</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>BRUSSELS</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1996-08-22</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>European</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index        word ner_tag\n",
       "0       1          EU   B-ORG\n",
       "1       2     rejects       O\n",
       "2       3      German  B-MISC\n",
       "3       4        call       O\n",
       "4       5          to       O\n",
       "5       6     boycott       O\n",
       "6       7     British  B-MISC\n",
       "7       8        lamb       O\n",
       "8       9           .       O\n",
       "9       1       Peter   B-PER\n",
       "10      2   Blackburn   I-PER\n",
       "11      1    BRUSSELS   B-LOC\n",
       "12      2  1996-08-22       O\n",
       "13      1         The       O\n",
       "14      2    European   B-ORG"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "99000c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight cleaning on num:\n",
    "train_df[\"word_formatted\"] = train_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].str.replace(r'^\\d+|.\\d+$', \"<num>\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5157aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of each word:\n",
    "#word-type = word\n",
    "cnt_d = {}\n",
    "for row in train_df.iterrows():\n",
    "    if row[1][\"word\"] in cnt_d:\n",
    "        cnt_d[row[1][\"word\"]] += 1\n",
    "    else:\n",
    "        cnt_d[row[1][\"word\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "48f0a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2 #No threshold = 1\n",
    "#unknown_cnt = 0\n",
    "unknown_word_lst = []   #We want to keep track of unknown words but group together\n",
    "for k, v in cnt_d.items():\n",
    "    if v < threshold:\n",
    "        #unknown_cnt += v\n",
    "        unknown_word_lst.append(k)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6a660cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_train(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5340f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_dev(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    elif word not in train_words:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f5d8ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Train:\n",
    "train_df[\"word_formatted\"] = train_df[\"word\"].apply(replace_unk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8b287a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word_formatted\"] = train_df[\"word_formatted\"].astype(str)\n",
    "train_vocab_size = len(np.unique(train_df[\"word_formatted\"]))\n",
    "train_words = np.unique(train_df[\"word_formatted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "68dde807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Dev:\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word\"].apply(replace_unk_dev)\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word_formatted\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "816b2996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>word_formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CRICKET</td>\n",
       "      <td>O</td>\n",
       "      <td>CRICKET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>LEICESTERSHIRE</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>TAKE</td>\n",
       "      <td>O</td>\n",
       "      <td>TAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>OVER</td>\n",
       "      <td>O</td>\n",
       "      <td>OVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51573</th>\n",
       "      <td>1</td>\n",
       "      <td>--</td>\n",
       "      <td>O</td>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51574</th>\n",
       "      <td>2</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>Dhaka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51575</th>\n",
       "      <td>3</td>\n",
       "      <td>Newsroom</td>\n",
       "      <td>I-ORG</td>\n",
       "      <td>Newsroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51576</th>\n",
       "      <td>4</td>\n",
       "      <td>880-2-506363</td>\n",
       "      <td>O</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51577</th>\n",
       "      <td>1</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>O</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51578 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index            word ner_tag word_formatted\n",
       "0          1         CRICKET       O        CRICKET\n",
       "1          2               -       O              -\n",
       "2          3  LEICESTERSHIRE   B-ORG          <unk>\n",
       "3          4            TAKE       O           TAKE\n",
       "4          5            OVER       O           OVER\n",
       "...      ...             ...     ...            ...\n",
       "51573      1              --       O             --\n",
       "51574      2           Dhaka   B-ORG          Dhaka\n",
       "51575      3        Newsroom   I-ORG       Newsroom\n",
       "51576      4    880-2-506363       O          <unk>\n",
       "51577      1      -DOCSTART-       O     -DOCSTART-\n",
       "\n",
       "[51578 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "09f83a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TRAIN:\n",
    "def format_data(df):\n",
    "    train_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    sentence_y = [first_word[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        #print(row)\n",
    "        if row[1][\"index\"] == 1:\n",
    "            #print(row[1][\"word\"])\n",
    "            train_formatted.append([sentence_x, sentence_y])\n",
    "\n",
    "            sentence_x, sentence_y = [], []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3409f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TEST:\n",
    "def format_data_test(df):\n",
    "    test_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(sentence_x)\n",
    "\n",
    "            sentence_x = []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                test_formatted.append(sentence_x)\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d04a956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted = format_data(train_df)\n",
    "dev_formatted = format_data(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "57f8914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for vocab:\n",
    "word_map = {\"<pad>\":0}\n",
    "for i, word in enumerate(set(train_df[\"word_formatted\"])):\n",
    "    word_map[word] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e418bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for ner_tag:\n",
    "ner_map = {\"<pad>\":-1}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "34291cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_map_without_pad = {}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map_without_pad[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f91bab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_train_sent = 0\n",
    "for sentence in train_formatted:\n",
    "    sentence_len = len(sentence[0])\n",
    "    if sentence_len > longest_train_sent:\n",
    "        longest_train_sent = sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0ff4c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "13f5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_sentences(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    print(cnt)\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "82078275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567\n",
      "51578\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sentences(train_formatted)\n",
    "dev_padded = pad_sentences(dev_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f25e80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_test_sentences(sentences_formatted):\n",
    "    test_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        cnt += len(sentence)\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    print(cnt)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712824b2",
   "metadata": {},
   "source": [
    "### Task 1: Simple Bidirectional LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b472a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "340df0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.shape)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        #linear_out_dropped = self.dropout(out)\n",
    "        #l2_out = self.linear2(linear_out_dropped)\n",
    "        elu_out = self.elu(out)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "04322e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.048592 \tTest Loss: 0.038024\n",
      "Epoch-1 lr: 0.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-a1628f8cd4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Perform backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Perform optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCAB_SIZE = train_vocab_size+1 #added <pad> word\n",
    "n_epochs = 20\n",
    "trainloader = torch.utils.data.DataLoader(train_padded, batch_size=16, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=16, num_workers=1)\n",
    "blstm = BLSTM(VOCAB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(blstm.parameters(), lr=0.25, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm.state_dict(), 'blstm1.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "0947aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "#Format the test_data:\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(replace_unk_dev)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted = format_data_test(test_df)\n",
    "test_padded = pad_test_sentences(test_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5434da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in the best model from above:\n",
    "blstm.load_state_dict(torch.load('blstm1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f0bb49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict_test(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e16a8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8c8507d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions_test(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        non_padded_pred = len(np.nonzero(sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6859b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        actual_sentence = sentence[0]\n",
    "        non_padded_pred = len(np.nonzero(actual_sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6d40a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(pred):\n",
    "    overall_pred = []\n",
    "    for sentence in pred:\n",
    "        for idx in sentence:\n",
    "            overall_pred.append(list(ner_map_without_pad.keys())[idx])\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f3f09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    score = sum(y_true == y_pred)/len(y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a3d96f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b6b77641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.8532513862499516\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7398e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Best score: lr=.25, gamm=.5, step=5, momentum=.9, epoch=20, batch_size-16 - 92.45% acc, 59.55 F1, ~2hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "569c4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(name, y_true, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[1][\"word\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(y_true[row[0]])\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ea7d71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev1.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "0fdd3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test:\n",
    "testloader = torch.utils.data.DataLoader(test_padded, batch_size=1, num_workers=1)\n",
    "predictions_test = predict_test(blstm, testloader)\n",
    "predictions_test = unravel_predictions_test(test_padded, predictions_test)\n",
    "predictions_test = convert_predictions(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "29c04390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_test(name, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[1][\"word\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "51f5a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_test(\"test1.out\", predictions_test, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d411468",
   "metadata": {},
   "source": [
    "### Task 2: Using GloVe Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3dd4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./glove.6B.100d\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    word2vec={}\n",
    "    for word_embedding in f:\n",
    "        word_split = word_embedding.split()\n",
    "        word = word_split[0]\n",
    "        word2vec[word] = np.array(word_split[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logic: Scratch previous <unk> and <num> tokens as Glove might handle it, vs. would set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5ef9fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for vocab:\n",
    "word_map_2 = {}\n",
    "for i, word in enumerate(set(train_df[\"word\"]).union(set(dev_df[\"word\"]))):\n",
    "    word_map_2[word] = i+1\n",
    "word_map_2[\"<unk>\"] = i+1 #leave last row to represent <unk>\n",
    "#Expand vocab to cover dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "411e9002",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 101\n",
    "VOCAB_SIZE = len(word_map_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f6b078c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in word_map.items():\n",
    "    if word in word2vec:\n",
    "        word_embedding = word2vec[word]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [0])) #final character 0 means lowercase\n",
    "    elif word.lower() in word2vec: #Attempt to solve case insensitive\n",
    "        word_embedding = word2vec[word.lower()]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [1])) #final character 1 means uppercase\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "embedding_blstm2 = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM) \n",
    "embedding_blstm2.load_state_dict({\"weight\": torch.tensor(embedding_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "936ec5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Have to fix padding from before:\n",
    "#Format the data by sentences TRAIN:\n",
    "def format_data_glove(df):\n",
    "    train_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word\"]]\n",
    "    sentence_y = [first_word[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        #print(row)\n",
    "        if row[1][\"index\"] == 1:\n",
    "            #print(row[1][\"word\"])\n",
    "            train_formatted.append([sentence_x, sentence_y])\n",
    "\n",
    "            sentence_x, sentence_y = [], []\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "daf82517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TEST:\n",
    "def format_data_test_glove(df):\n",
    "    test_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(sentence_x)\n",
    "\n",
    "            sentence_x = []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                test_formatted.append(sentence_x)\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5742e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted_glove = format_data_glove(train_df)\n",
    "dev_formatted_glove = format_data_glove(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "96985d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_test_glove(word):\n",
    "    if word in word_map_2:\n",
    "        return word\n",
    "    else:\n",
    "        return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "47bef013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the test_data Glove:\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(replace_unk_test_glove)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted_glove = format_data_test_glove(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "49f57fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_sentences_glove(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    print(cnt)\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "93aef059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567\n",
      "51578\n"
     ]
    }
   ],
   "source": [
    "train_padded_glove = pad_sentences_glove(train_formatted_glove)\n",
    "dev_padded_glove = pad_sentences_glove(dev_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "451f23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_test_sentences_glove(sentences_formatted):\n",
    "    test_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        cnt += len(sentence)\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    print(cnt)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d4b48c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "test_padded_glove = pad_test_sentences_glove(test_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4587622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(input_size=101, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.shape)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        #print(embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        #linear_out_dropped = self.dropout(out)\n",
    "        #l2_out = self.linear2(linear_out_dropped)\n",
    "        elu_out = self.elu(out)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e2e64029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.001\n",
      "Epoch: 1 \tTraining Loss: 0.099626 \tTest Loss: 0.071618\n",
      "Epoch-1 lr: 0.001\n",
      "Epoch: 2 \tTraining Loss: 0.068658 \tTest Loss: 0.066757\n",
      "Epoch-2 lr: 0.001\n",
      "Epoch: 3 \tTraining Loss: 0.062385 \tTest Loss: 0.060342\n",
      "Epoch-3 lr: 0.001\n",
      "Epoch: 4 \tTraining Loss: 0.054688 \tTest Loss: 0.053673\n",
      "Epoch-4 lr: 0.001\n",
      "Epoch: 5 \tTraining Loss: 0.047851 \tTest Loss: 0.049047\n",
      "Epoch-5 lr: 0.001\n",
      "Epoch: 6 \tTraining Loss: 0.043159 \tTest Loss: 0.046144\n",
      "Epoch-6 lr: 0.001\n",
      "Epoch: 7 \tTraining Loss: 0.040122 \tTest Loss: 0.044400\n",
      "Epoch-7 lr: 0.001\n",
      "Epoch: 8 \tTraining Loss: 0.038091 \tTest Loss: 0.043286\n",
      "Epoch-8 lr: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-2a96d95ead09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Perform backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Perform optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 101\n",
    "VOCAB_SIZE = len(word_map_2)\n",
    "n_epochs = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_padded_glove, batch_size=12, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=12, num_workers=1)\n",
    "blstm2 = BLSTM_2(embedding_blstm2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(blstm2.parameters(), lr=0.01, momentum=0.7)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    #scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm2.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm2(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm2(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm2.state_dict(), 'blstm2.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "5b899b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the best model from the given run:\n",
    "blstm2.load_state_dict(torch.load('blstm2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "688df2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm2, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "cd9dcdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.8637597425258832\n"
     ]
    }
   ],
   "source": [
    "#Dev accuracy:\n",
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "e741edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev2.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b062cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "testloader = torch.utils.data.DataLoader(test_padded_glove, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_test = predict_test(blstm2, testloader)\n",
    "predictions_test = unravel_predictions_test(test_padded, predictions_test)\n",
    "predictions_test = convert_predictions(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "d2759002",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_test(\"test2.out\", predictions_test, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN Command line:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
