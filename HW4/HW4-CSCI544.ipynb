{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c5cb6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import warnings\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4924631",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data:\n",
    "tr_headers = [\"index\", \"word\", \"ner_tag\"]\n",
    "train_df = pd.read_csv(\"./data/train\", sep=' ', header=None, quoting=3)\n",
    "train_df.columns = tr_headers\n",
    "\n",
    "dev_df = pd.read_csv(\"./data/dev\", sep=' ', header=None, quoting=3)\n",
    "dev_df.columns = tr_headers\n",
    "\n",
    "test_headers = [\"index\", \"word\"]\n",
    "test_df = pd.read_csv(\"./data/test\", sep=' ', header=None, engine='python', error_bad_lines=False, quoting=3)\n",
    "test_df.columns = test_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03298400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>28</td>\n",
       "      <td>advice</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>29</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>30</td>\n",
       "      <td>clearer</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>31</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>\"</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2</td>\n",
       "      <td>We</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     word ner_tag\n",
       "70     28   advice       O\n",
       "71     29      was       O\n",
       "72     30  clearer       O\n",
       "73     31        .       O\n",
       "74      1        \"       O\n",
       "75      2       We       O"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[70:76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "10d22a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>boycott</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>British</td>\n",
       "      <td>B-MISC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>lamb</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Peter</td>\n",
       "      <td>B-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Blackburn</td>\n",
       "      <td>I-PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>BRUSSELS</td>\n",
       "      <td>B-LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1996-08-22</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>European</td>\n",
       "      <td>B-ORG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index        word ner_tag\n",
       "0       1          EU   B-ORG\n",
       "1       2     rejects       O\n",
       "2       3      German  B-MISC\n",
       "3       4        call       O\n",
       "4       5          to       O\n",
       "5       6     boycott       O\n",
       "6       7     British  B-MISC\n",
       "7       8        lamb       O\n",
       "8       9           .       O\n",
       "9       1       Peter   B-PER\n",
       "10      2   Blackburn   I-PER\n",
       "11      1    BRUSSELS   B-LOC\n",
       "12      2  1996-08-22       O\n",
       "13      1         The       O\n",
       "14      2    European   B-ORG"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5cc5eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word_formatted\"] = train_df[\"word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d81cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[np.nonzero(train_df[\"word_formatted\"].str.contains(r'\\d', regex=True).values)[0],3] = \"<num>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df0f9bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>word_formatted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EU</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>rejects</td>\n",
       "      <td>O</td>\n",
       "      <td>rejects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>German</td>\n",
       "      <td>B-MISC</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>call</td>\n",
       "      <td>O</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204562</th>\n",
       "      <td>1</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>Swansea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204563</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "      <td>&lt;num&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204564</th>\n",
       "      <td>3</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>Lincoln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204565</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>&lt;num&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204566</th>\n",
       "      <td>1</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "      <td>O</td>\n",
       "      <td>-DOCSTART-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204567 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        index        word ner_tag word_formatted\n",
       "0           1          EU   B-ORG             EU\n",
       "1           2     rejects       O        rejects\n",
       "2           3      German  B-MISC         German\n",
       "3           4        call       O           call\n",
       "4           5          to       O             to\n",
       "...       ...         ...     ...            ...\n",
       "204562      1     Swansea   B-ORG        Swansea\n",
       "204563      2           1       O          <num>\n",
       "204564      3     Lincoln   B-ORG        Lincoln\n",
       "204565      4           2       O          <num>\n",
       "204566      1  -DOCSTART-       O     -DOCSTART-\n",
       "\n",
       "[204567 rows x 4 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "99000c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slight cleaning on num:\n",
    "train_df[\"word_formatted\"] = train_df[\"word\"]\n",
    "train_df.iloc[np.nonzero(train_df[\"word_formatted\"].str.contains(r'\\d', regex=True).values)[0],3] = \"<num>\"\n",
    "\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word\"]\n",
    "dev_df.iloc[np.nonzero(dev_df[\"word_formatted\"].str.contains(r'\\d', regex=True).values)[0],3] = \"<num>\"\n",
    "\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"]\n",
    "test_df.iloc[np.nonzero(test_df[\"word_formatted\"].str.contains(r'\\d', regex=True).values)[0],2] = \"<num>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5157aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of each word:\n",
    "#word-type = word\n",
    "cnt_d = {}\n",
    "for row in train_df.iterrows():\n",
    "    if row[1][\"word\"] in cnt_d:\n",
    "        cnt_d[row[1][\"word\"]] += 1\n",
    "    else:\n",
    "        cnt_d[row[1][\"word\"]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48f0a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2 #No threshold = 1\n",
    "#unknown_cnt = 0\n",
    "unknown_word_lst = []   #We want to keep track of unknown words but group together\n",
    "for k, v in cnt_d.items():\n",
    "    if v < threshold:\n",
    "        #unknown_cnt += v\n",
    "        unknown_word_lst.append(k)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a660cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_train(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5340f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_dev(word):\n",
    "    if word in unknown_word_lst:\n",
    "        return \"<unk>\"\n",
    "    elif word not in train_words:\n",
    "        return \"<unk>\"\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5d8ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Train:\n",
    "train_df[\"word_formatted\"] = train_df[\"word\"].apply(replace_unk_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b287a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"word_formatted\"] = train_df[\"word_formatted\"].astype(str)\n",
    "train_vocab_size = len(np.unique(train_df[\"word_formatted\"]))\n",
    "train_words = np.unique(train_df[\"word_formatted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68dde807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace with <unk> Dev:\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word\"].apply(replace_unk_dev)\n",
    "dev_df[\"word_formatted\"] = dev_df[\"word_formatted\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09f83a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TRAIN:\n",
    "def format_data(df):\n",
    "    train_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    sentence_y = [first_word[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        #print(row)\n",
    "        if row[1][\"index\"] == 1:\n",
    "            #print(row[1][\"word\"])\n",
    "            train_formatted.append([sentence_x, sentence_y])\n",
    "\n",
    "            sentence_x, sentence_y = [], []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3409f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TEST:\n",
    "def format_data_test(df):\n",
    "    test_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(sentence_x)\n",
    "\n",
    "            sentence_x = []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                test_formatted.append(sentence_x)\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d04a956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted = format_data(train_df)\n",
    "dev_formatted = format_data(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57f8914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for vocab:\n",
    "word_map = {\"<pad>\":0}\n",
    "for i, word in enumerate(set(train_df[\"word_formatted\"])):\n",
    "    word_map[word] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e418bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for ner_tag:\n",
    "ner_map = {\"<pad>\":-1}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34291cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_map_without_pad = {}\n",
    "for i, word in enumerate(set(train_df[\"ner_tag\"])):\n",
    "    ner_map_without_pad[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f91bab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_train_sent = 0\n",
    "for sentence in train_formatted:\n",
    "    sentence_len = len(sentence[0])\n",
    "    if sentence_len > longest_train_sent:\n",
    "        longest_train_sent = sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ff4c257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_train_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13f5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_sentences(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    print(cnt)\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82078275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567\n",
      "51578\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sentences(train_formatted)\n",
    "dev_padded = pad_sentences(dev_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f25e80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_test_sentences(sentences_formatted):\n",
    "    test_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        cnt += len(sentence)\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    print(cnt)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712824b2",
   "metadata": {},
   "source": [
    "### Task 1: Simple Bidirectional LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b472a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "340df0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, 100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.shape)\n",
    "        embeds = self.embeddings(inputs)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        #linear_out_dropped = self.dropout(out)\n",
    "        #l2_out = self.linear2(linear_out_dropped)\n",
    "        elu_out = self.elu(out)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04322e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.25\n",
      "Epoch: 1 \tTraining Loss: 0.047948 \tTest Loss: 0.037853\n",
      "Epoch-1 lr: 0.25\n",
      "Epoch: 2 \tTraining Loss: 0.035913 \tTest Loss: 0.030399\n",
      "Epoch-2 lr: 0.125\n",
      "Epoch: 3 \tTraining Loss: 0.025182 \tTest Loss: 0.025678\n",
      "Epoch-3 lr: 0.125\n",
      "Epoch: 4 \tTraining Loss: 0.021110 \tTest Loss: 0.023839\n",
      "Epoch-4 lr: 0.125\n",
      "Epoch: 5 \tTraining Loss: 0.018458 \tTest Loss: 0.022544\n",
      "Epoch-5 lr: 0.0625\n",
      "Epoch: 6 \tTraining Loss: 0.016178 \tTest Loss: 0.022248\n",
      "Epoch-6 lr: 0.0625\n",
      "Epoch: 7 \tTraining Loss: 0.015095 \tTest Loss: 0.021921\n",
      "Epoch-7 lr: 0.0625\n",
      "Epoch: 8 \tTraining Loss: 0.014558 \tTest Loss: 0.021378\n",
      "Epoch-8 lr: 0.03125\n",
      "Epoch: 9 \tTraining Loss: 0.013768 \tTest Loss: 0.021088\n",
      "Epoch-9 lr: 0.03125\n",
      "Epoch: 10 \tTraining Loss: 0.013401 \tTest Loss: 0.020697\n",
      "Epoch-10 lr: 0.03125\n",
      "Epoch: 11 \tTraining Loss: 0.013171 \tTest Loss: 0.020598\n",
      "Epoch-11 lr: 0.015625\n",
      "Epoch: 12 \tTraining Loss: 0.012719 \tTest Loss: 0.020306\n",
      "Epoch-12 lr: 0.015625\n",
      "Epoch: 13 \tTraining Loss: 0.012596 \tTest Loss: 0.020154\n",
      "Epoch-13 lr: 0.015625\n",
      "Epoch: 14 \tTraining Loss: 0.012509 \tTest Loss: 0.020212\n",
      "Epoch-14 lr: 0.0078125\n",
      "Epoch: 15 \tTraining Loss: 0.012324 \tTest Loss: 0.019808\n",
      "Epoch-15 lr: 0.0078125\n",
      "Epoch: 16 \tTraining Loss: 0.012221 \tTest Loss: 0.019659\n",
      "Epoch-16 lr: 0.0078125\n",
      "Epoch: 17 \tTraining Loss: 0.012217 \tTest Loss: 0.019606\n",
      "Epoch-17 lr: 0.00390625\n",
      "Epoch: 18 \tTraining Loss: 0.012099 \tTest Loss: 0.019380\n",
      "Epoch-18 lr: 0.00390625\n",
      "Epoch: 19 \tTraining Loss: 0.012021 \tTest Loss: 0.019427\n",
      "Epoch-19 lr: 0.00390625\n",
      "Epoch: 20 \tTraining Loss: 0.012012 \tTest Loss: 0.019425\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "VOCAB_SIZE = train_vocab_size+1 #added <pad> word\n",
    "n_epochs = 20\n",
    "trainloader = torch.utils.data.DataLoader(train_padded, batch_size=16, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=16, num_workers=1)\n",
    "blstm = BLSTM(VOCAB_SIZE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(blstm.parameters(), lr=0.25, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm.state_dict(), 'blstm1.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0947aa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "#Format the test_data:\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(replace_unk_dev)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted = format_data_test(test_df)\n",
    "test_padded = pad_test_sentences(test_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5434da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the best model from the given run:\n",
    "blstm1 = BLSTM(VOCAB_SIZE)\n",
    "blstm1.load_state_dict(torch.load('blstm1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0bb49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict_test(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e16a8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(torch.cat(data,dim=0))\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c8507d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions_test(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        non_padded_pred = len(np.nonzero(sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6859b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_predictions(data, pred):\n",
    "    overall_pred = []\n",
    "    for i, sentence in enumerate(data):\n",
    "        actual_sentence = sentence[0]\n",
    "        non_padded_pred = len(np.nonzero(actual_sentence)[0])\n",
    "        pred_i = pred[i].tolist()[0:non_padded_pred]\n",
    "        overall_pred.append(pred_i)\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d40a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(pred):\n",
    "    overall_pred = []\n",
    "    for sentence in pred:\n",
    "        for idx in sentence:\n",
    "            overall_pred.append(list(ner_map_without_pad.keys())[idx])\n",
    "    return overall_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9f3f09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    score = sum(y_true == y_pred)/len(y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3d96f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm1, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6b77641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.9207801775950987\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9973747",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Best score: lr=.25, gamm=.5, step=5, momentum=.9, epoch=20, batch_size-16 - 92.45% acc, 59.55 F1, ~2hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "569c4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(name, y_true, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[1][\"word\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(y_true[row[0]])\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea7d71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev1.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0fdd3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test:\n",
    "testloader = torch.utils.data.DataLoader(test_padded, batch_size=1, num_workers=1)\n",
    "predictions_test = predict_test(blstm1, testloader)\n",
    "predictions_test = unravel_predictions_test(test_padded, predictions_test)\n",
    "predictions_test = convert_predictions(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33916d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_test(name, y_pred, df):\n",
    "    with open(name, 'w') as f:\n",
    "        for row in df.iloc[0:].iterrows():\n",
    "            f.write(str(row[1][\"index\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[1][\"word\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(y_pred[row[0]])\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bdc5589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_test(\"test1.out\", predictions_test, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9ed2c",
   "metadata": {},
   "source": [
    "### Task 2: Using GloVe Word Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca53a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./glove.6B.100d\",\"r\",encoding=\"UTF-8\") as f:\n",
    "    word2vec={}\n",
    "    for word_embedding in f:\n",
    "        word_split = word_embedding.split()\n",
    "        word = word_split[0]\n",
    "        word2vec[word] = np.array(word_split[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logic: Scratch previous <unk> and <num> tokens as Glove might handle it, vs. would set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3c1d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Word Map for vocab:\n",
    "word_map_2 = {}\n",
    "for i, word in enumerate(set(train_df[\"word\"]).union(set(dev_df[\"word\"]))):\n",
    "    word_map_2[word] = i+1\n",
    "word_map_2[\"<unk>\"] = i+1 #leave last row to represent <unk>\n",
    "#Expand vocab to cover dev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd6fa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 101\n",
    "VOCAB_SIZE = len(word_map_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34622910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, idx in word_map.items():\n",
    "    if word in word2vec:\n",
    "        word_embedding = word2vec[word]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [0])) #final character 0 means lowercase\n",
    "    elif word.lower() in word2vec: #Attempt to solve case insensitive\n",
    "        word_embedding = word2vec[word.lower()]\n",
    "        embedding_matrix[idx,:] = np.concatenate((word_embedding, [1])) #final character 1 means uppercase\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "embedding_blstm2 = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM) \n",
    "embedding_blstm2.load_state_dict({\"weight\": torch.tensor(embedding_matrix)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c1aae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Have to fix padding from before:\n",
    "#Format the data by sentences TRAIN:\n",
    "def format_data_glove(df):\n",
    "    train_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word\"]]\n",
    "    sentence_y = [first_word[\"ner_tag\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        #print(row)\n",
    "        if row[1][\"index\"] == 1:\n",
    "            #print(row[1][\"word\"])\n",
    "            train_formatted.append([sentence_x, sentence_y])\n",
    "\n",
    "            sentence_x, sentence_y = [], []\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                train_formatted.append([sentence_x, sentence_y])\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word\"])\n",
    "            sentence_y.append(row[1][\"ner_tag\"])\n",
    "    return train_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3a16022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the data by sentences TEST:\n",
    "def format_data_test_glove(df):\n",
    "    test_formatted = []\n",
    "    #init beginning:\n",
    "    first_word = df.iloc[0]\n",
    "    sentence_x = [first_word[\"word_formatted\"]]\n",
    "    \n",
    "    for row in df.iloc[1:].iterrows():\n",
    "        if row[1][\"index\"] == 1:\n",
    "            test_formatted.append(sentence_x)\n",
    "\n",
    "            sentence_x = []\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "            if row[0] == (df.shape[0]-1):\n",
    "                test_formatted.append(sentence_x)\n",
    "        else:\n",
    "            sentence_x.append(row[1][\"word_formatted\"])\n",
    "    \n",
    "    return test_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35f94173",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted_glove = format_data_glove(train_df)\n",
    "dev_formatted_glove = format_data_glove(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98139481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unk_test_glove(word):\n",
    "    if word in word_map_2:\n",
    "        return word\n",
    "    else:\n",
    "        return \"<unk>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e61b9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the test_data Glove:\n",
    "test_df[\"word_formatted\"] = test_df[\"word\"].apply(replace_unk_test_glove)\n",
    "test_df[\"word_formatted\"] = test_df[\"word_formatted\"].astype(str)\n",
    "\n",
    "test_formatted_glove = format_data_test_glove(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33d901ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_sentences_glove(sentences_formatted):\n",
    "    train_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        word_lst = sentence[0]\n",
    "        ner_lst = sentence[1]\n",
    "        mapped_word_lst, mapped_ner_lst = [], []\n",
    "        cnt += len(word_lst)\n",
    "        for word in word_lst:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "        for ner in ner_lst:\n",
    "            mapped_ner_lst.append(ner_map[ner])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "        mapped_ner_lst = mapped_ner_lst + [-1] * diff_\n",
    "\n",
    "        train_padded.append([mapped_word_lst, mapped_ner_lst])\n",
    "    print(cnt)\n",
    "    return train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4dbeeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567\n",
      "51578\n"
     ]
    }
   ],
   "source": [
    "train_padded_glove = pad_sentences_glove(train_formatted_glove)\n",
    "dev_padded_glove = pad_sentences_glove(dev_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58424de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map words in sentences to corresponding values:\n",
    "def pad_test_sentences_glove(sentences_formatted):\n",
    "    test_padded = []\n",
    "    cnt = 0\n",
    "    for sentence in sentences_formatted:\n",
    "        mapped_word_lst = []\n",
    "        cnt += len(sentence)\n",
    "        for word in sentence:\n",
    "            mapped_word_lst.append(word_map_2[word])\n",
    "\n",
    "        word_cnt = len(mapped_word_lst)\n",
    "        diff_ = longest_train_sent - word_cnt\n",
    "        mapped_word_lst = mapped_word_lst + [0] * diff_\n",
    "\n",
    "        test_padded.append(mapped_word_lst)\n",
    "    print(cnt)\n",
    "    return test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc9c5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46666\n"
     ]
    }
   ],
   "source": [
    "test_padded_glove = pad_test_sentences_glove(test_formatted_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf417736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM_2(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_hidden_dim = 256\n",
    "        lstm_num_layers = 1\n",
    "        linear_output_dim =128\n",
    "        output_dim = 10\n",
    "        \n",
    "        self.embeddings = embeddings\n",
    "        self.lstm = nn.LSTM(input_size=101, hidden_size=256,\n",
    "                          num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.linear1 = nn.Linear(512, 128)\n",
    "        self.elu = nn.ELU()\n",
    "        self.linear2 = nn.Linear(128, 9)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(inputs), 1, -1))\n",
    "        lstm_out_dropped = self.dropout(lstm_out)\n",
    "        out = self.linear1(lstm_out_dropped.view(len(inputs), -1))\n",
    "        elu_out = self.elu(out)\n",
    "        l2_out = self.linear2(elu_out)\n",
    "        log_probs = F.log_softmax(l2_out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a082f4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0 lr: 0.3\n",
      "Epoch: 1 \tTraining Loss: 0.007482 \tTest Loss: 0.010769\n",
      "Epoch-1 lr: 0.3\n",
      "Epoch: 2 \tTraining Loss: 0.004435 \tTest Loss: 0.010506\n",
      "Epoch-2 lr: 0.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-835f4cd8c68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print(torch.cat(data,dim=0).reshape(1,400,316))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#print(torch.cat(data,dim=0).size(0)) I think the problem is here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblstm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-2108b373c67d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0membeds_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#print(embeds.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds_dropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mlstm_out_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out_dropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    680\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 101\n",
    "VOCAB_SIZE = len(word_map_2)\n",
    "n_epochs = 20\n",
    "trainloader = torch.utils.data.DataLoader(train_padded_glove, batch_size=16, num_workers=1)\n",
    "devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=16, num_workers=1)\n",
    "blstm2 = BLSTM_2(embedding_blstm2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1, size_average=True) #therefore no need for softmax\n",
    "#criterion = nn.NLLLoss()\n",
    "#optimizer = torch.optim.Adam(blstm2.parameters(), lr=.01)\n",
    "optimizer = torch.optim.SGD(blstm2.parameters(), lr=0.25, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "test_loss_min = 10000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    blstm2.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "#         print(data)\n",
    "#         print(target)\n",
    "        #print(torch.cat(data,dim=0).reshape(1,400,316))\n",
    "        #print(torch.cat(data,dim=0).size(0)) I think the problem is here.\n",
    "        output = blstm2(torch.cat(data,dim=0))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, torch.cat(target,dim=0))\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "                # Print statistics\n",
    "        #train_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "        train_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in devloader:\n",
    "            output = blstm2(torch.cat(data,dim=0))\n",
    "            loss = criterion(output, torch.cat(target,dim=0))\n",
    "#             test_loss += loss.item()*torch.cat(data,dim=0).size(0)\n",
    "            test_loss += loss\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(devloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(blstm2.state_dict(), 'blstm2.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4b38d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the best model from the given run:\n",
    "blstm2 = BLSTM_2(embedding_blstm2)\n",
    "blstm2.load_state_dict(torch.load('blstm2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b59c1aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "devloader = torch.utils.data.DataLoader(dev_padded_glove, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_dev = predict(blstm2, devloader)\n",
    "predictions_dev = unravel_predictions(dev_padded, predictions_dev)\n",
    "predictions_dev = convert_predictions(predictions_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d5c8d006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51578\n",
      "51578\n",
      "Dev Accuracy: 0.9214975377098763\n"
     ]
    }
   ],
   "source": [
    "#Dev accuracy:\n",
    "y_true = np.array(dev_df[\"ner_tag\"])\n",
    "print(len(y_true))\n",
    "print(len(predictions_dev))\n",
    "print(\"Dev Accuracy:\", accuracy(y_true, predictions_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "499afb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results(\"dev2.out\", y_true, predictions_dev, dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ceb6c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on dev:\n",
    "testloader = torch.utils.data.DataLoader(test_padded_glove, batch_size=1, num_workers=1) #need to do 1 at a time:\n",
    "predictions_test = predict_test(blstm2, testloader)\n",
    "predictions_test = unravel_predictions_test(test_padded, predictions_test)\n",
    "predictions_test = convert_predictions(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "4d7e3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_test(\"test2.out\", predictions_test, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN Command line:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
