{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "788dd885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import warnings\n",
    "from platform import python_version\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd0caf5",
   "metadata": {},
   "source": [
    "### Read Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc922dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could not load it directly from the url:\n",
    "ratings_df = pd.read_csv(\"./data/amazon_reviews_us_Kitchen_v1_00.tsv\", sep=\"\\t\",\n",
    "                         error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9fb62595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>37000337</td>\n",
       "      <td>R3DT59XH7HXR9K</td>\n",
       "      <td>B00303FI0G</td>\n",
       "      <td>529320574</td>\n",
       "      <td>Arthur Court Paper Towel Holder</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Beautiful. Looks great on counter</td>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>15272914</td>\n",
       "      <td>R1LFS11BNASSU8</td>\n",
       "      <td>B00JCZKZN6</td>\n",
       "      <td>274237558</td>\n",
       "      <td>Olde Thompson Bavaria Glass Salt and Pepper Mi...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Awesome &amp; Self-ness</td>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>36137863</td>\n",
       "      <td>R296RT05AG0AF6</td>\n",
       "      <td>B00JLIKA5C</td>\n",
       "      <td>544675303</td>\n",
       "      <td>Progressive International PL8 Professional Man...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Fabulous and worth every penny</td>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>43311049</td>\n",
       "      <td>R3V37XDZ7ZCI3L</td>\n",
       "      <td>B000GBNB8G</td>\n",
       "      <td>491599489</td>\n",
       "      <td>Zyliss Jumbo Garlic Press</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>13763148</td>\n",
       "      <td>R14GU232NQFYX2</td>\n",
       "      <td>B00VJ5KX9S</td>\n",
       "      <td>353790155</td>\n",
       "      <td>1 X Premier Pizza Cutter - Stainless Steel 14\"...</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Better than sex</td>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     37000337  R3DT59XH7HXR9K  B00303FI0G       529320574   \n",
       "1          US     15272914  R1LFS11BNASSU8  B00JCZKZN6       274237558   \n",
       "2          US     36137863  R296RT05AG0AF6  B00JLIKA5C       544675303   \n",
       "3          US     43311049  R3V37XDZ7ZCI3L  B000GBNB8G       491599489   \n",
       "4          US     13763148  R14GU232NQFYX2  B00VJ5KX9S       353790155   \n",
       "\n",
       "                                       product_title product_category  \\\n",
       "0                    Arthur Court Paper Towel Holder          Kitchen   \n",
       "1  Olde Thompson Bavaria Glass Salt and Pepper Mi...          Kitchen   \n",
       "2  Progressive International PL8 Professional Man...          Kitchen   \n",
       "3                          Zyliss Jumbo Garlic Press          Kitchen   \n",
       "4  1 X Premier Pizza Cutter - Stainless Steel 14\"...          Kitchen   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0          5.0            0.0          0.0    N                 Y   \n",
       "1          5.0            0.0          1.0    N                 Y   \n",
       "2          5.0            0.0          0.0    N                 Y   \n",
       "3          5.0            0.0          1.0    N                 Y   \n",
       "4          5.0            0.0          0.0    N                 Y   \n",
       "\n",
       "                     review_headline  \\\n",
       "0  Beautiful. Looks great on counter   \n",
       "1                Awesome & Self-ness   \n",
       "2     Fabulous and worth every penny   \n",
       "3                         Five Stars   \n",
       "4                    Better than sex   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                Beautiful.  Looks great on counter.  2015-08-31  \n",
       "1  I personally have 5 days sets and have also bo...  2015-08-31  \n",
       "2  Fabulous and worth every penny. Used for clean...  2015-08-31  \n",
       "3  A must if you love garlic on tomato marinara s...  2015-08-31  \n",
       "4  Worth every penny! Buy one now and be a pizza ...  2015-08-31  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aeb04e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beautiful.  Looks great on counter.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I personally have 5 days sets and have also bo...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fabulous and worth every penny. Used for clean...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A must if you love garlic on tomato marinara s...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Worth every penny! Buy one now and be a pizza ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0                Beautiful.  Looks great on counter.          5.0\n",
       "1  I personally have 5 days sets and have also bo...          5.0\n",
       "2  Fabulous and worth every penny. Used for clean...          5.0\n",
       "3  A must if you love garlic on tomato marinara s...          5.0\n",
       "4  Worth every penny! Buy one now and be a pizza ...          5.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplify Dataset: Ensure all have reviews\n",
    "ratings_df = ratings_df.loc[:, [\"review_body\", \"star_rating\"]]\n",
    "ratings_df = ratings_df[ratings_df[\"review_body\"].notnull()]\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c4cdae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering why the coffee started tasting...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have had this pan for 10 years.  It is well-...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good seal</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Variable grind and a decent size bean hopper, ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great pan for small pizzas.</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating\n",
       "0  I was wondering why the coffee started tasting...          1.0\n",
       "1  I have had this pan for 10 years.  It is well-...          5.0\n",
       "2                                          good seal          4.0\n",
       "3  Variable grind and a decent size bean hopper, ...          3.0\n",
       "4                        Great pan for small pizzas.          4.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather 50k of each rating through random selection:\n",
    "rating_1 = ratings_df[ratings_df[\"star_rating\"] == 1].sample(50000)\n",
    "rating_2 = ratings_df[ratings_df[\"star_rating\"] == 2].sample(50000)\n",
    "rating_3 = ratings_df[ratings_df[\"star_rating\"] == 3].sample(50000)\n",
    "rating_4 = ratings_df[ratings_df[\"star_rating\"] == 4].sample(50000)\n",
    "rating_5 = ratings_df[ratings_df[\"star_rating\"] == 5].sample(50000)\n",
    "\n",
    "ratings_sampled_df = pd.concat([rating_1, rating_2, rating_3, rating_4, rating_5])\n",
    "ratings_sampled_df = ratings_sampled_df.sample(frac=1)\n",
    "ratings_sampled_df.reset_index(drop=True, inplace=True)\n",
    "ratings_sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "641b424b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering why the coffee started tasting...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have had this pan for 10 years.  It is well-...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good seal</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Variable grind and a decent size bean hopper, ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great pan for small pizzas.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  star_rating  sentiment\n",
       "0  I was wondering why the coffee started tasting...          1.0          0\n",
       "1  I have had this pan for 10 years.  It is well-...          5.0          1\n",
       "2                                          good seal          4.0          1\n",
       "3  Variable grind and a decent size bean hopper, ...          3.0          2\n",
       "4                        Great pan for small pizzas.          4.0          1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map star rating to sentiment rating: We will assign -1 as neutral\n",
    "d_ = {4:1, 5:1, 1:0, 2:0, 3:2}\n",
    "ratings_sampled_df[\"sentiment\"] = ratings_sampled_df[\"star_rating\"].map(d_)\n",
    "ratings_sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39e346c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test Split\n",
    "train, test = train_test_split(ratings_sampled_df, test_size=0.2)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1f5ae",
   "metadata": {},
   "source": [
    "### Word Embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6162d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the word2vec:\n",
    "import gensim\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f24991ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.844939112663269),\n",
       " ('queen', 0.7300516366958618),\n",
       " ('monarch', 0.6454660296440125)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Smeantic Similarities: (close enough!)\n",
    "result_vec = word2vec[\"king\"] - word2vec[\"man\"] + word2vec[\"woman\"]\n",
    "word2vec.most_similar(positive=result_vec, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0dcdf5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrific', 0.7409728765487671),\n",
       " ('superb', 0.7062715888023376),\n",
       " ('exceptional', 0.681470513343811)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(positive=\"excellent\", topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19c8cbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-2b2b707881ec>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review_body\"] = train[\"review_body\"].str.lower()\n",
      "<ipython-input-74-2b2b707881ec>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review_body\"] = test[\"review_body\"].str.lower()\n",
      "<ipython-input-74-2b2b707881ec>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review_body\"] = train[\"review_body\"].replace(r'http\\S+|www.\\S+', '', regex=True)\n",
      "<ipython-input-74-2b2b707881ec>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review_body\"] = test[\"review_body\"].replace(r'http\\S+|www.\\S+', '', regex=True)\n",
      "<ipython-input-74-2b2b707881ec>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review_body\"] = train[\"review_body\"].replace(r'[^a-z|\\s]', '', regex=True)\n",
      "<ipython-input-74-2b2b707881ec>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review_body\"] = test[\"review_body\"].replace(r'[^a-z|\\s]', '', regex=True)\n",
      "<ipython-input-74-2b2b707881ec>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review_body\"] = train[\"review_body\"].replace(r'\\s\\s+', ' ', regex=True)\n",
      "<ipython-input-74-2b2b707881ec>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review_body\"] = test[\"review_body\"].replace(r'\\s\\s+', ' ', regex=True)\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model using own dataset: embedding_size=300, window_size=11, min_word_cnt=10\n",
    "# Clean data:\n",
    "train[\"review_body\"] = train[\"review_body\"].str.lower()\n",
    "test[\"review_body\"] = test[\"review_body\"].str.lower()\n",
    "\n",
    "train[\"review_body\"] = train[\"review_body\"].replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "test[\"review_body\"] = test[\"review_body\"].replace(r'http\\S+|www.\\S+', '', regex=True)\n",
    "\n",
    "\n",
    "train[\"review_body\"] = train[\"review_body\"].replace(r'[^a-z|\\s]', '', regex=True)\n",
    "test[\"review_body\"] = test[\"review_body\"].replace(r'[^a-z|\\s]', '', regex=True)\n",
    "\n",
    "train[\"review_body\"] = train[\"review_body\"].replace(r'\\s\\s+', ' ', regex=True)\n",
    "test[\"review_body\"] = test[\"review_body\"].replace(r'\\s\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6594d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-7e488c7e9935>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"review_body\"] = train[\"review_body\"].apply(remove_stopwords)\n",
      "<ipython-input-75-7e488c7e9935>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test[\"review_body\"] = test[\"review_body\"].apply(remove_stopwords)\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    tokens = word_tokenize(s)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "train[\"review_body\"] = train[\"review_body\"].apply(remove_stopwords)\n",
    "test[\"review_body\"] = test[\"review_body\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "753b587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = np.array(train[\"review_body\"])\n",
    "test_sentences = np.array(test[\"review_body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c9040e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(train_sentences, min_count=10, vector_size=200, window=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b37576be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.668415367603302),\n",
       " ('deluxe', 0.4885542392730713),\n",
       " ('compatible', 0.47892406582832336),\n",
       " ('stainlesssteel', 0.47053804993629456),\n",
       " ('ii', 0.4653341472148895),\n",
       " ('xx', 0.45887693762779236),\n",
       " ('differences', 0.44889411330223083),\n",
       " ('burnished', 0.44834545254707336),\n",
       " ('identical', 0.4480072557926178),\n",
       " ('catalog', 0.4453065097332001)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test semantic similarities:\n",
    "result_vec = model.wv[\"king\"] - model.wv[\"man\"] + model.wv[\"woman\"]\n",
    "model.wv.similar_by_vector(result_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "302a66c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outstanding', 0.7804235219955444),\n",
       " ('fantastic', 0.7426757216453552),\n",
       " ('exceptional', 0.6960605382919312),\n",
       " ('superb', 0.6956508159637451),\n",
       " ('wonderful', 0.6925870180130005),\n",
       " ('terrific', 0.6809049844741821),\n",
       " ('great', 0.6170509457588196),\n",
       " ('superior', 0.5734264254570007),\n",
       " ('fabulous', 0.5658342838287354),\n",
       " ('amazing', 0.5618612766265869)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"excellent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680cd21c",
   "metadata": {},
   "source": [
    "From the Semantic Similarities above we can see that our model actually performs relatively well given the results of the top 10 closest words to Excellent. However we can see the problem emerges that the reviews do not necessarily contain all the words we might want such as \"Queen\", which may or may not be in the data based on the randomness involved in selecting reviews. One would prefer to utilize the all encompassing Google word2vec to help with unorthodox words, and we can trust that the performance is better. Yet, the create w2v does produce results faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ba3c5",
   "metadata": {},
   "source": [
    "### Simple Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f4fdd",
   "metadata": {},
   "source": [
    "#### Personally Trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2a22bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to make binary model at this point:\n",
    "train_binary = train[~(train[\"sentiment\"] == 2)]\n",
    "test_binary = test[~(test[\"sentiment\"] == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1533761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_binary[\"review_body\"]\n",
    "Y_train = train_binary[\"sentiment\"].values\n",
    "\n",
    "X_test = test_binary[\"review_body\"]\n",
    "Y_test = test_binary[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68bf262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector_mine(review):\n",
    "    tot_ = 0\n",
    "    for word_ in review:\n",
    "        if word_ in model.wv:\n",
    "            tot_ += model.wv[word_]\n",
    "        else:\n",
    "            continue\n",
    "    if len(review) == 0:\n",
    "        return 0\n",
    "    return tot_/len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa42fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_train:\n",
    "X_train_avg = X_train.apply(avg_vector_mine)\n",
    "X_train_df = pd.DataFrame.from_dict(dict(zip(X_train_avg.index, X_train_avg.values))).T\n",
    "X_train_vectorized_mine = X_train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e37354b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_test:\n",
    "X_test_avg = X_test.apply(avg_vector_mine)\n",
    "X_test_df = pd.DataFrame.from_dict(dict(zip(X_test_avg.index, X_test_avg.values))).T\n",
    "X_test_vectorized_mine = X_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "855fde1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Training Accuracy: 0.7960728471857903\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(tol=1e-3, random_state=42)\n",
    "perceptron.fit(X_train_vectorized_mine, Y_train)\n",
    "print(\"Perceptron Training Accuracy:\", perceptron.score(X_train_vectorized_mine, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87c9d56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Test Accuracy: 0.7959582790091264\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron Test Accuracy:\", perceptron.score(X_test_vectorized_mine, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e88aa549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy: 0.8586837043143721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(max_iter=1000)\n",
    "svm.fit(X_train_vectorized_mine, Y_train)\n",
    "print(\"SVM Training Accuracy:\", svm.score(X_train_vectorized_mine, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81cd5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test Accuracy: 0.8573864206197974\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Test Accuracy:\", svm.score(X_test_vectorized_mine, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c068b4",
   "metadata": {},
   "source": [
    "#### Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad7af0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_vector_google(review):\n",
    "    tot_ = 0\n",
    "    for word_ in review:\n",
    "        if word_ in word2vec:\n",
    "            tot_ += word2vec[word_]\n",
    "        else:\n",
    "            continue\n",
    "    if len(review) == 0:\n",
    "        return 0\n",
    "    return tot_/len(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fa0c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_train:\n",
    "X_train_avg = X_train.apply(avg_vector_google)\n",
    "X_train_df = pd.DataFrame.from_dict(dict(zip(X_train_avg.index, X_train_avg.values))).T\n",
    "X_train_vectorized_google = X_train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08175016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_test:\n",
    "X_test_avg = X_test.apply(avg_vector_google)\n",
    "X_test_df = pd.DataFrame.from_dict(dict(zip(X_test_avg.index, X_test_avg.values))).T\n",
    "X_test_vectorized_google = X_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3125c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Training Accuracy: 0.8067401134177721\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(tol=1e-3, random_state=42)\n",
    "perceptron.fit(X_train_vectorized_google, Y_train)\n",
    "print(\"Perceptron Training Accuracy:\", perceptron.score(X_train_vectorized_google, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e462c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Test Accuracy: 0.8041570554608364\n"
     ]
    }
   ],
   "source": [
    "print(\"Perceptron Test Accuracy:\", perceptron.score(X_test_vectorized_google, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c35fd00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy: 0.8208236528516826\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(max_iter=1000)\n",
    "svm.fit(X_train_vectorized_google, Y_train)\n",
    "print(\"SVM Training Accuracy:\", svm.score(X_train_vectorized_google, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c814c1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Test Accuracy: 0.817420519506569\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Test Accuracy:\", svm.score(X_test_vectorized_google, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc469872",
   "metadata": {},
   "source": [
    "Perceptron Results TF.IDF: Test-Accuracy = .8569 <br>\n",
    "SVM Results TF.IDF: Test-Accuracy = 0.898025 <br>\n",
    "<br>\n",
    "We can see from the above results that the models are better for TF.IDF, then personally trained Word2Vec, and lastly for the Google pre-trained Word2Vec. One likely explanation for this could be that the TF.IDF and personally train Word2Vec might be more tailored to the specific review set at hand. In addition, the averaging of the word vectors might be a very inefficient way to combine words in a review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65115c3c",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ee36b",
   "metadata": {},
   "source": [
    "#### Avg Word2Vec Vectors - Personally Trained Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2844d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffb88774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dim = 200\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.act3 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 200)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de0182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the data for DataLoader (coverts to tensors):\n",
    "train_data = []\n",
    "for i in range(len(X_train_vectorized_mine)):\n",
    "    train_data.append([X_train_vectorized_mine[i], Y_train[i]])\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test_vectorized_mine)):\n",
    "    test_data.append([X_test_vectorized_mine[i], Y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "902d9739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.389826 \tTest Loss: 0.351972\n",
      "Epoch: 2 \tTraining Loss: 0.345981 \tTest Loss: 0.344050\n",
      "Epoch: 3 \tTraining Loss: 0.337811 \tTest Loss: 0.335044\n",
      "Epoch: 4 \tTraining Loss: 0.330687 \tTest Loss: 0.332909\n",
      "Epoch: 5 \tTraining Loss: 0.327175 \tTest Loss: 0.327461\n",
      "Epoch: 6 \tTraining Loss: 0.323126 \tTest Loss: 0.325748\n",
      "Epoch: 7 \tTraining Loss: 0.320272 \tTest Loss: 0.326852\n",
      "Epoch: 8 \tTraining Loss: 0.318014 \tTest Loss: 0.323711\n",
      "Epoch: 9 \tTraining Loss: 0.315877 \tTest Loss: 0.323428\n",
      "Epoch: 10 \tTraining Loss: 0.312822 \tTest Loss: 0.322309\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=1)\n",
    "mlp = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01)\n",
    "\n",
    "test_loss_min = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    mlp.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        output = mlp(data.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = mlp(data.float())\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(testloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(mlp.state_dict(), 'model.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff581e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.load_state_dict(torch.load('model.pt'))\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1152c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Accuracy from trained model:\n",
    "def predict(model, dataloader):\n",
    "    prediction_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(data.float())\n",
    "            _, predicted = torch.max(output.data, 1) \n",
    "            prediction_list.append(predicted)\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3974d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(mlp, testloader)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6216a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    score = sum(y_true == y_pred)/len(y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45825173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8674907230969813\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce4608be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting process for 3 class:\n",
    "X_train_3 = train[\"review_body\"]\n",
    "Y_train_3 = train[\"sentiment\"].values\n",
    "\n",
    "X_test_3 = test[\"review_body\"]\n",
    "Y_test_3 = test[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b2e5813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_train_3:\n",
    "X_train_avg_3 = X_train_3.apply(avg_vector_mine)\n",
    "X_train_df_3 = pd.DataFrame.from_dict(dict(zip(X_train_avg_3.index, X_train_avg_3.values))).T\n",
    "X_train_vectorized_mine_3 = X_train_df_3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3228279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_test_3:\n",
    "X_test_avg_3 = X_test_3.apply(avg_vector_mine)\n",
    "X_test_df_3 = pd.DataFrame.from_dict(dict(zip(X_test_avg_3.index, X_test_avg_3.values))).T\n",
    "X_test_vectorized_mine_3 = X_test_df_3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3b5e1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the data for DataLoader (coverts to tensors):\n",
    "train_data_3 = []\n",
    "for i in range(len(X_train_vectorized_mine_3)):\n",
    "    train_data_3.append([X_train_vectorized_mine_3[i], Y_train_3[i]])\n",
    "    \n",
    "test_data_3 = []\n",
    "for i in range(len(X_test_vectorized_mine_3)):\n",
    "    test_data_3.append([X_test_vectorized_mine_3[i], Y_test_3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5bd77827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dim = 200\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 200)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bcfe65d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.796609 \tTest Loss: 0.747008\n",
      "Epoch: 2 \tTraining Loss: 0.735844 \tTest Loss: 0.735304\n",
      "Epoch: 3 \tTraining Loss: 0.725607 \tTest Loss: 0.727630\n",
      "Epoch: 4 \tTraining Loss: 0.718142 \tTest Loss: 0.724393\n",
      "Epoch: 5 \tTraining Loss: 0.714715 \tTest Loss: 0.722115\n",
      "Epoch: 6 \tTraining Loss: 0.710518 \tTest Loss: 0.720172\n",
      "Epoch: 7 \tTraining Loss: 0.708163 \tTest Loss: 0.716973\n",
      "Epoch: 8 \tTraining Loss: 0.705964 \tTest Loss: 0.713493\n",
      "Epoch: 9 \tTraining Loss: 0.701447 \tTest Loss: 0.713009\n",
      "Epoch: 10 \tTraining Loss: 0.698704 \tTest Loss: 0.711250\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_data_3, batch_size=20, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(test_data_3, batch_size=20, num_workers=1)\n",
    "mlp_3 = MLP_3()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(mlp.parameters(), lr=.0001)\n",
    "optimizer = torch.optim.SGD(mlp_3.parameters(), lr=0.01)\n",
    "\n",
    "test_loss_min = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    mlp_3.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        output = mlp_3(data.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = mlp_3(data.float())\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(testloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(mlp_3.state_dict(), 'model_3.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "882f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_3.load_state_dict(torch.load('model_3.pt'))\n",
    "testloader = torch.utils.data.DataLoader(test_data_3, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "826558a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(mlp_3, testloader)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "de3e7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6971\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy(Y_test_3, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5764da",
   "metadata": {},
   "source": [
    "#### Avg Word2Vec Vectors - Google Trained Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0f58eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the data for DataLoader (coverts to tensors):\n",
    "train_data = []\n",
    "for i in range(len(X_train_vectorized_google)):\n",
    "    train_data.append([X_train_vectorized_google[i], Y_train[i]])\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test_vectorized_google)):\n",
    "    test_data.append([X_test_vectorized_google[i], Y_test[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3e474e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Google(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dim = 300\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #self.act3 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2e660337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.657763 \tTest Loss: 0.530660\n",
      "Epoch: 2 \tTraining Loss: 0.482147 \tTest Loss: 0.460093\n",
      "Epoch: 3 \tTraining Loss: 0.452953 \tTest Loss: 0.448825\n",
      "Epoch: 4 \tTraining Loss: 0.440136 \tTest Loss: 0.435709\n",
      "Epoch: 5 \tTraining Loss: 0.430075 \tTest Loss: 0.431090\n",
      "Epoch: 6 \tTraining Loss: 0.424777 \tTest Loss: 0.426827\n",
      "Epoch: 7 \tTraining Loss: 0.420459 \tTest Loss: 0.429685\n",
      "Epoch: 8 \tTraining Loss: 0.417830 \tTest Loss: 0.428514\n",
      "Epoch: 9 \tTraining Loss: 0.415146 \tTest Loss: 0.425935\n",
      "Epoch: 10 \tTraining Loss: 0.413365 \tTest Loss: 0.423997\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=1)\n",
    "mlp_g = MLP_Google()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(mlp_g.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(mlp_g.parameters(), lr=0.01)\n",
    "\n",
    "test_loss_min = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    mlp_g.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        output = mlp_g(data.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = mlp_g(data.float())\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(testloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(mlp_g.state_dict(), 'model_google.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c800d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_g.load_state_dict(torch.load('model_google.pt'))\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a042c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(mlp_g, testloader)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "525d0f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.809973924380704\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab3475ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_train_3:\n",
    "X_train_avg_3 = X_train_3.apply(avg_vector_google)\n",
    "X_train_df_3 = pd.DataFrame.from_dict(dict(zip(X_train_avg_3.index, X_train_avg_3.values))).T\n",
    "X_train_vectorized_google_3 = X_train_df_3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "941a931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X_test_3:\n",
    "X_test_avg_3 = X_test_3.apply(avg_vector_google)\n",
    "X_test_df_3 = pd.DataFrame.from_dict(dict(zip(X_test_avg_3.index, X_test_avg_3.values))).T\n",
    "X_test_vectorized_google_3 = X_test_df_3.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the data for DataLoader (coverts to tensors):\n",
    "train_data = []\n",
    "for i in range(len(X_train_vectorized_google_3)):\n",
    "    train_data.append([X_train_vectorized_google_3[i], Y_train_3[i]])\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test_vectorized_google_3)):\n",
    "    test_data.append([X_test_vectorized_google_3[i], Y_test_3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce714c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Google_3(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_dim = 300\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_1)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.fc3 = nn.Linear(hidden_2, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 300)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "cb6dba3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.617759 \tTest Loss: 0.488001\n",
      "Epoch: 2 \tTraining Loss: 0.466237 \tTest Loss: 0.452972\n",
      "Epoch: 3 \tTraining Loss: 0.446832 \tTest Loss: 0.443936\n",
      "Epoch: 4 \tTraining Loss: 0.438610 \tTest Loss: 0.439470\n",
      "Epoch: 5 \tTraining Loss: 0.433162 \tTest Loss: 0.435645\n",
      "Epoch: 6 \tTraining Loss: 0.428534 \tTest Loss: 0.430747\n",
      "Epoch: 7 \tTraining Loss: 0.423448 \tTest Loss: 0.428099\n",
      "Epoch: 8 \tTraining Loss: 0.421179 \tTest Loss: 0.430775\n",
      "Epoch: 9 \tTraining Loss: 0.417592 \tTest Loss: 0.424281\n",
      "Epoch: 10 \tTraining Loss: 0.415827 \tTest Loss: 0.428536\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=20, num_workers=1)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=1)\n",
    "mlp_g_3 = MLP_Google_3()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(mlp_g.parameters(), lr=1e-4)\n",
    "optimizer = torch.optim.SGD(mlp_g_3.parameters(), lr=0.01)\n",
    "\n",
    "test_loss_min = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    \n",
    "    mlp_g_3.train()\n",
    "    for data, target in trainloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        output = mlp_g_3(data.float())\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            output = mlp_g_3(data.float())\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    test_loss = test_loss/len(testloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        test_loss\n",
    "        ))\n",
    "    \n",
    "    if test_loss <= test_loss_min:\n",
    "        torch.save(mlp_g_3.state_dict(), 'model_google_3.pt')\n",
    "        test_loss_min = test_loss\n",
    "\n",
    "  # Process is complete.\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cc0308eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_g_3.load_state_dict(torch.load('model_google_3.pt'))\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3620b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(mlp_g_3, testloader)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "634507ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8077173804031692\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", accuracy(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cff49",
   "metadata": {},
   "source": [
    "### Binary & Ternary Models For First 10 W2V Vectors Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c07467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
